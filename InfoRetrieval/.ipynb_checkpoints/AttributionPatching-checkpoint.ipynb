{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554246ea-a3a5-4ba0-9913-fb1ad996954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /work/pi_jensen_umass_edu/svaidyanatha_umass_edu/code-llm-entity-tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5288ad6-b002-4590-bbc1-a021be98d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5dcb5a-734f-40f9-95c6-713f20f6b000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2389705/3715311821.py:33: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "/tmp/ipykernel_2389705/3715311821.py:34: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "from typing import List, Union, Optional, Callable\n",
    "from typing_extensions import Literal\n",
    "from functools import partial\n",
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML, Markdown\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "ipython = get_ipython()\n",
    "ipython.magic(\"load_ext autoreload\")\n",
    "ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abaa3e57-7cfb-4249-aaa7-e3d79123143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS A LOCAL (MODIFIED) VERSION OF TRANSFORMER_LENS - UNINSTALL PIP/CONDA VERSION BEFORE USE!\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "import transformer_lens.patching as patching\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6615a520-aa6c-4751-bb8e-61c5fe537ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace bookkeeping so that the home folder doesn't get filled up\n",
    "# I have these in my .bashrc and .zshrc and they work in terminal but not recognized by python for some reason\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "hf_path = \"/work/pi_jensen_umass_edu/svaidyanatha_umass_edu/huggingface\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = hf_path\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = hf_path\n",
    "os.environ[\"HF_HOME\"] = hf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59f5d5aa-7be1-4c24-a0f5-c75f2161f2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 2\n",
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 0.00 MB  Reserved: 0.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 0.00 MB  Reserved: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# When using multiple GPUs we use GPU 0 as the primary and switch to the next when it is 90% full\n",
    "num_gpus = torch.cuda.device_count()\n",
    "device_id = 0\n",
    "if num_gpus > 0:\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "def check_gpu_memory(max_alloc=0.9):\n",
    "    if not torch.cuda.is_available():\n",
    "        return\n",
    "    global device_id, device\n",
    "    print(\"Primary device:\", device)\n",
    "    torch.cuda.empty_cache()\n",
    "    max_alloc = 1 if max_alloc > 1 else max_alloc\n",
    "    for gpu in range(num_gpus):\n",
    "        memory_reserved = torch.cuda.memory_reserved(device=gpu)\n",
    "        memory_allocated = torch.cuda.memory_allocated(device=gpu)\n",
    "        total_memory = torch.cuda.get_device_properties(gpu).total_memory \n",
    "        print(f\"GPU {gpu}: {total_memory / (1024**2):.2f} MB  Allocated: {memory_allocated / (1024**2):.2f} MB  Reserved: {memory_reserved / (1024**2):.2f} MB\")\n",
    "                \n",
    "        # Check if the current GPU is getting too full, and if so we switch the primary device to the next GPU\n",
    "        if memory_reserved > max_alloc * total_memory:\n",
    "            if device_id < num_gpus - 1:\n",
    "                device_id += 1\n",
    "                device = f\"cuda:{device_id}\"\n",
    "                print(f\"Switching primary device to {device}\")\n",
    "            else:\n",
    "                print(\"Cannot switch primary device, all GPUs are nearly full\")\n",
    "\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b756d9ab-2d79-46ba-9f07-7e80355dedeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77280d50b94f4a4ca631465cede13c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model CodeLlama-7b-hf into HookedTransformer\n",
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 13174.18 MB  Reserved: 13194.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 13174.30 MB  Reserved: 13194.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Create transformer\n",
    "model = HookedTransformer.from_pretrained(\"CodeLlama-7b-hf\", n_devices=num_gpus)\n",
    "\n",
    "# We need these so that individual attention heads and MLP inputs can be edited\n",
    "model.set_use_attn_in(True)\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_attn_result(True) # Documentation says this easily burns through GPU memory\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d894051d-9a86-4d8e-9fc0-23d2d2c02bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean string 0 <s> When J and M went to the shops, J gave the bag to \n",
      "Corrupted string 0 <s> When J and M went to the shops, M gave the bag to \n",
      "Answer token indices tensor([[341, 435],\n",
      "        [435, 341],\n",
      "        [323, 435],\n",
      "        [435, 323],\n",
      "        [360, 317],\n",
      "        [317, 360],\n",
      "        [390, 319],\n",
      "        [319, 390]], device='cuda:0')\n",
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 13174.18 MB  Reserved: 13194.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 13174.30 MB  Reserved: 13194.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Some examples to work with - replace with code examples\n",
    "\n",
    "prompts = ['When J and M went to the shops, J gave the bag to ', 'When J and M went to the shops, M gave the bag to ', 'When T and J went to the park, J gave the ball to ', 'When T and J went to the park, T gave the ball to ', 'When D and S went to the shops, S gave an apple to ', 'When D and S went to the shops, D gave an apple to ', 'After R and A went to the park, A gave a drink to ', 'After R and A went to the park, R gave a drink to ']\n",
    "answers = [('M', 'J'), ('J', 'M'), ('T', 'J'), ('J', 'T'), ('D', 'S'), ('S', 'D'), ('R', 'A'), ('A', 'R')]\n",
    "\n",
    "clean_tokens = model.to_tokens(prompts)\n",
    "# Swap each adjacent pair, with a hacky list comprehension\n",
    "corrupted_tokens = clean_tokens[\n",
    "    [(i+1 if i%2==0 else i-1) for i in range(len(clean_tokens)) ]\n",
    "    ]\n",
    "print(\"Clean string 0\", model.to_string(clean_tokens[0]))\n",
    "print(\"Corrupted string 0\", model.to_string(corrupted_tokens[0]))\n",
    "\n",
    "answer_token_indices = torch.tensor([[model.to_single_token(answers[i][j]) for j in range(2)] for i in range(len(answers))], device=device)\n",
    "print(\"Answer token indices\", answer_token_indices)\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4453f87-017c-468b-ac5c-97641992c7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit diff: 0.9658\n",
      "Corrupted logit diff: -0.9658\n",
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 25923.93 MB  Reserved: 26028.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 25963.34 MB  Reserved: 26054.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Logit difference metric\n",
    "\n",
    "def get_logit_diff(logits, answer_token_indices=answer_token_indices):\n",
    "    if len(logits.shape) == 3:\n",
    "        # Get final logits only\n",
    "        logits = logits[:, -1, :]\n",
    "    logits = logits.to(answer_token_indices.device)\n",
    "    correct_logits = logits.gather(1, answer_token_indices[:, 0].unsqueeze(1))\n",
    "    incorrect_logits = logits.gather(1, answer_token_indices[:, 1].unsqueeze(1))\n",
    "    return (correct_logits - incorrect_logits).mean()\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "\n",
    "clean_logit_diff = get_logit_diff(clean_logits, answer_token_indices).item()\n",
    "print(f\"Clean logit diff: {clean_logit_diff:.4f}\")\n",
    "\n",
    "corrupted_logit_diff = get_logit_diff(corrupted_logits, answer_token_indices).item()\n",
    "print(f\"Corrupted logit diff: {corrupted_logit_diff:.4f}\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "517b36a9-7ff8-42d1-9380-d67fce09e6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Baseline is 1: 1.0000\n",
      "Corrupted Baseline is 0: 0.0000\n",
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 25923.93 MB  Reserved: 26028.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 25963.34 MB  Reserved: 26054.00 MB\n"
     ]
    }
   ],
   "source": [
    "# IOI metric - does this mean indirect object identification? Yes\n",
    "\n",
    "CLEAN_BASELINE = clean_logit_diff\n",
    "CORRUPTED_BASELINE = corrupted_logit_diff\n",
    "\n",
    "def ioi_metric(logits, answer_token_indices=answer_token_indices):\n",
    "    logits = logits.to(device)\n",
    "    torch.cuda.empty_cache()\n",
    "    return (get_logit_diff(logits, answer_token_indices) - CORRUPTED_BASELINE) / (\n",
    "        CLEAN_BASELINE - CORRUPTED_BASELINE\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"Clean Baseline is 1: {ioi_metric(clean_logits).item():.4f}\")\n",
    "print(f\"Corrupted Baseline is 0: {ioi_metric(corrupted_logits).item():.4f}\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29f3130d-3ec4-423a-abc2-69beec2b967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metric = Callable[[TT[\"batch_and_pos_dims\", \"d_model\"]], float]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52c3bf5-fc32-4a5b-94ed-ef3984f1f11e",
   "metadata": {},
   "source": [
    "## Removing prev cache\n",
    "The memory shot up during run_with_cache, which I dont think we need later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "229423cc-b22d-4b2a-93f0-782bacfa3bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 13182.31 MB  Reserved: 13194.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 13182.43 MB  Reserved: 13194.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Delete tensors\n",
    "del clean_logits\n",
    "del corrupted_logits\n",
    "del clean_logit_diff \n",
    "del corrupted_logit_diff\n",
    "del clean_cache\n",
    "del corrupted_cache\n",
    "\n",
    "# Empty CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Optionally check memory to confirm\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a4b13-698e-4802-a378-cf3ab900f551",
   "metadata": {},
   "source": [
    "# Attribution patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2256c670-9c9c-4fcc-95e4-4e7ddd682804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Value: 0.9999999403953552\n",
      "Clean Activations Cached: 739\n",
      "Clean Gradients Cached: 739\n",
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 40844.35 MB  Reserved: 40936.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 26045.30 MB  Reserved: 26078.00 MB\n",
      "Corrupted Value: 0.0\n",
      "Corrupted Activations Cached: 739\n",
      "Corrupted Gradients Cached: 739\n",
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 55651.41 MB  Reserved: 55802.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 26045.30 MB  Reserved: 26078.00 MB\n"
     ]
    }
   ],
   "source": [
    "filter_not_qkv_input = lambda name: \"_input\" not in name\n",
    "\n",
    "def get_cache_fwd_and_bwd(model, tokens, metric):\n",
    "    model.reset_hooks()\n",
    "    cache = {}\n",
    "\n",
    "    def forward_cache_hook(act, hook):\n",
    "        act = act.to(device)\n",
    "        torch.cuda.empty_cache()\n",
    "        cache[hook.name] = act\n",
    "\n",
    "    model.add_hook(filter_not_qkv_input, forward_cache_hook, \"fwd\")\n",
    "    grad_cache = {}\n",
    "\n",
    "    def backward_cache_hook(act, hook):\n",
    "        act = act.to(device)\n",
    "        torch.cuda.empty_cache()\n",
    "        grad_cache[hook.name] = act\n",
    "\n",
    "    model.add_hook(filter_not_qkv_input, backward_cache_hook, \"bwd\")\n",
    "    \n",
    "    result = model(tokens).to(device)\n",
    "    torch.cuda.empty_cache()\n",
    "    value = metric(result)\n",
    "    value.backward()\n",
    "\n",
    "    # Reset hooks and clear unused GPU memory\n",
    "    value = value.item()\n",
    "    model.reset_hooks()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    cache = ActivationCache(cache, model).to(device)\n",
    "    grad_cache = ActivationCache(grad_cache, model).to(device)\n",
    "    \n",
    "    return value,cache, grad_cache\n",
    "\n",
    "clean_value, clean_cache, clean_grad_cache = get_cache_fwd_and_bwd(model, clean_tokens, ioi_metric)\n",
    "print(\"Clean Value:\", clean_value)\n",
    "print(\"Clean Activations Cached:\", len(clean_cache))\n",
    "print(\"Clean Gradients Cached:\", len(clean_grad_cache))\n",
    "check_gpu_memory()\n",
    "\n",
    "corrupted_value, corrupted_cache, corrupted_grad_cache = get_cache_fwd_and_bwd(model, corrupted_tokens, ioi_metric)\n",
    "print(\"Corrupted Value:\", corrupted_value)\n",
    "print(\"Corrupted Activations Cached:\", len(corrupted_cache))\n",
    "print(\"Corrupted Gradients Cached:\", len(corrupted_grad_cache))\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f471e187-555c-40c1-8623-424ac16df547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformer_lens.ActivationCache.ActivationCache'>\n"
     ]
    }
   ],
   "source": [
    "print(type(clean_cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aff66c26-1abb-44c1-b90f-2aa4a948a6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 26044.60 MB  Reserved: 26090.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 26045.30 MB  Reserved: 26078.00 MB\n"
     ]
    }
   ],
   "source": [
    "clean_cache = clean_cache.to('cpu')\n",
    "clean_grad_cache = clean_grad_cache.to('cpu')\n",
    "corrupted_cache = corrupted_cache.to('cpu')\n",
    "corrupted_grad_cache = corrupted_grad_cache.to('cpu')\n",
    "\n",
    "# Optionally, if you have any other variables on GPU, move them to CPU in similar fashion\n",
    "# example_tensor = example_tensor.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check GPU memory\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "150cb74d-2ee3-4e9b-887d-7b0deb545a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L0H0', 'L0H1', 'L0H2', 'L0H3', 'L0H4']\n",
      "['L0H0+', 'L0H0-', 'L0H1+', 'L0H1-', 'L0H2+']\n",
      "['L0H0Q', 'L0H0K', 'L0H0V', 'L0H1Q', 'L0H1K']\n"
     ]
    }
   ],
   "source": [
    "HEAD_NAMES = [\n",
    "    f\"L{l}H{h}\" for l in range(model.cfg.n_layers) for h in range(model.cfg.n_heads)\n",
    "]\n",
    "HEAD_NAMES_SIGNED = [f\"{name}{sign}\" for name in HEAD_NAMES for sign in [\"+\", \"-\"]]\n",
    "HEAD_NAMES_QKV = [\n",
    "    f\"{name}{act_name}\" for name in HEAD_NAMES for act_name in [\"Q\", \"K\", \"V\"]\n",
    "]\n",
    "print(HEAD_NAMES[:5])\n",
    "print(HEAD_NAMES_SIGNED[:5])\n",
    "print(HEAD_NAMES_QKV[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86e5a643-dc88-4ab1-b94a-153d5fc46803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 26044.60 MB  Reserved: 26090.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 26045.30 MB  Reserved: 26078.00 MB\n"
     ]
    }
   ],
   "source": [
    "def create_attention_attr(\n",
    "    clean_cache, clean_grad_cache, device\n",
    ") -> TT[\"batch\", \"layer\", \"head_index\", \"dest\", \"src\"]:\n",
    "    attention_stack = torch.stack(\n",
    "        [clean_cache[\"pattern\", l] for l in range(model.cfg.n_layers)], dim=0\n",
    "    ).to(device)\n",
    "    attention_grad_stack = torch.stack(\n",
    "        [clean_grad_cache[\"pattern\", l] for l in range(model.cfg.n_layers)], dim=0\n",
    "    ).to(device)\n",
    "    attention_attr = attention_grad_stack * attention_stack\n",
    "    attention_attr = einops.rearrange(\n",
    "        attention_attr,\n",
    "        \"layer batch head_index dest src -> batch layer head_index dest src\",\n",
    "    )\n",
    "    return attention_attr\n",
    "\n",
    "attention_attr = create_attention_attr(clean_cache, clean_grad_cache, \"cpu\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c765b5e-e5d2-4fbb-a08b-bf3603132917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 26044.60 MB  Reserved: 26090.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 26045.30 MB  Reserved: 26078.00 MB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a2fe9f-f172-4d41-952b-b833b3decb47",
   "metadata": {},
   "source": [
    "# Residual Stream Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86cbd6e1-25fd-458a-88a2-98b741f77242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attr_patch_residual(\n",
    "    clean_cache: ActivationCache,\n",
    "    corrupted_cache: ActivationCache,\n",
    "    corrupted_grad_cache: ActivationCache,\n",
    "    device,\n",
    ") -> TT[\"component\", \"pos\"]:\n",
    "    clean_residual, residual_labels = clean_cache.accumulated_resid(\n",
    "        -1, incl_mid=True, return_labels=True\n",
    "    )\n",
    "    corrupted_residual = corrupted_cache.accumulated_resid(\n",
    "        -1, incl_mid=True, return_labels=False\n",
    "    )\n",
    "    corrupted_grad_residual = corrupted_grad_cache.accumulated_resid(\n",
    "        -1, incl_mid=True, return_labels=False\n",
    "    )\n",
    "    residual_attr = einops.reduce(\n",
    "        corrupted_grad_residual * (clean_residual - corrupted_residual),\n",
    "        \"component batch pos d_model -> component pos\",\n",
    "        \"sum\",\n",
    "    ).to(device)\n",
    "    return residual_attr, residual_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7b78686-f90d-47ba-8102-0dc0e1502da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 26044.60 MB  Reserved: 26090.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 26045.30 MB  Reserved: 26078.00 MB\n"
     ]
    }
   ],
   "source": [
    "residual_attr, residual_labels = attr_patch_residual(\n",
    "    clean_cache, corrupted_cache, corrupted_grad_cache, \"cpu\"\n",
    ")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb85eecd-1f94-43ba-8eec-c882c3e29e0c",
   "metadata": {},
   "source": [
    "# Layer Output Attribution Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b0e9a8e-2ab6-44ea-b6e9-2fe397b2281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attr_patch_layer_out(\n",
    "    clean_cache: ActivationCache,\n",
    "    corrupted_cache: ActivationCache,\n",
    "    corrupted_grad_cache: ActivationCache,\n",
    "    device\n",
    ") -> TT[\"component\", \"pos\"]:\n",
    "    clean_layer_out, labels = clean_cache.decompose_resid(-1, return_labels=True)\n",
    "    corrupted_layer_out = corrupted_cache.decompose_resid(-1, return_labels=False)\n",
    "    corrupted_grad_layer_out = corrupted_grad_cache.decompose_resid(\n",
    "        -1, return_labels=False\n",
    "    )\n",
    "    layer_out_attr = einops.reduce(\n",
    "        corrupted_grad_layer_out * (clean_layer_out - corrupted_layer_out),\n",
    "        \"component batch pos d_model -> component pos\",\n",
    "        \"sum\",\n",
    "    ).to(device)\n",
    "    return layer_out_attr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa052b74-1ffa-4f17-ad62-1d4a38c77c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 26044.60 MB  Reserved: 26090.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 26045.30 MB  Reserved: 26078.00 MB\n"
     ]
    }
   ],
   "source": [
    "layer_out_attr, layer_out_labels = attr_patch_layer_out(\n",
    "    clean_cache, corrupted_cache, corrupted_grad_cache, \"cpu\"\n",
    ")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb51e6-18d2-4865-95f8-4c5f987d4494",
   "metadata": {},
   "source": [
    "# Head output attribution Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbb2dbc3-9991-4169-8b0e-b54ef738a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attr_patch_head_out(\n",
    "    clean_cache: ActivationCache,\n",
    "    corrupted_cache: ActivationCache,\n",
    "    corrupted_grad_cache: ActivationCache,\n",
    "    device\n",
    ") -> TT[\"component\", \"pos\"]:\n",
    "    labels = HEAD_NAMES\n",
    "\n",
    "    clean_head_out = clean_cache.stack_head_results(-1, return_labels=False).to(device)\n",
    "    corrupted_head_out = corrupted_cache.stack_head_results(-1, return_labels=False).to(device)\n",
    "    corrupted_grad_head_out = corrupted_grad_cache.stack_head_results(\n",
    "        -1, return_labels=False\n",
    "    ).to(device)\n",
    "    head_out_attr = einops.reduce(\n",
    "        corrupted_grad_head_out * (clean_head_out - corrupted_head_out),\n",
    "        \"component batch pos d_model -> component pos\",\n",
    "        \"sum\",\n",
    "    ).to(device)\n",
    "    return head_out_attr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b31566a2-c5ce-4698-9418-ca25b894f558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 26044.60 MB  Reserved: 26090.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 26045.30 MB  Reserved: 26078.00 MB\n"
     ]
    }
   ],
   "source": [
    "head_out_attr, head_out_labels = attr_patch_head_out(\n",
    "    clean_cache, corrupted_cache, corrupted_grad_cache, \"cpu\"\n",
    ")\n",
    "\n",
    "sum_head_out_attr = einops.reduce(\n",
    "    head_out_attr,\n",
    "    \"(layer head) pos -> layer head\",\n",
    "    \"sum\",\n",
    "    layer=model.cfg.n_layers,\n",
    "    head=model.cfg.n_heads,\n",
    ")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39523830-9748-4aed-bfd1-daeae5a99831",
   "metadata": {},
   "source": [
    "# Head activation patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5a80684-b1a2-44bd-931e-db98d2d5819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_head_vector_from_cache(\n",
    "    cache, activation_name: Literal[\"q\", \"k\", \"v\", \"z\"], device\n",
    ") -> TT[\"layer_and_head_index\", \"batch\", \"pos\", \"d_head\"]:\n",
    "    \"\"\"Stacks the head vectors from the cache from a specific activation (key, query, value or mixed_value (z)) into a single tensor.\"\"\"\n",
    "    stacked_head_vectors = torch.stack(\n",
    "        [cache[activation_name, l] for l in range(model.cfg.n_layers)], dim=0\n",
    "    ).to(device)\n",
    "    stacked_head_vectors = einops.rearrange(\n",
    "        stacked_head_vectors,\n",
    "        \"layer batch pos head_index d_head -> (layer head_index) batch pos d_head\",\n",
    "    ).to(device)\n",
    "    return stacked_head_vectors\n",
    "\n",
    "\n",
    "def attr_patch_head_vector(\n",
    "    clean_cache: ActivationCache,\n",
    "    corrupted_cache: ActivationCache,\n",
    "    corrupted_grad_cache: ActivationCache,\n",
    "    activation_name: Literal[\"q\", \"k\", \"v\", \"z\"],\n",
    "    device\n",
    ") -> TT[\"component\", \"pos\"]:\n",
    "    labels = HEAD_NAMES\n",
    "\n",
    "    clean_head_vector = stack_head_vector_from_cache(clean_cache, activation_name, \"cpu\").to(device)\n",
    "    corrupted_head_vector = stack_head_vector_from_cache(\n",
    "        corrupted_cache, activation_name, \"cpu\"\n",
    "    ).to(device)\n",
    "    corrupted_grad_head_vector = stack_head_vector_from_cache(\n",
    "        corrupted_grad_cache, activation_name, \"cpu\"\n",
    "    ).to(device)\n",
    "    head_vector_attr = einops.reduce(\n",
    "        corrupted_grad_head_vector * (clean_head_vector - corrupted_head_vector),\n",
    "        \"component batch pos d_head -> component pos\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    return head_vector_attr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d3fdbb1-7b60-4ba2-a263-062926db7689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 26044.60 MB  Reserved: 26090.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 26045.30 MB  Reserved: 26078.00 MB\n"
     ]
    }
   ],
   "source": [
    "head_vector_attr_dict = {}\n",
    "for activation_name, activation_name_full in [\n",
    "    (\"k\", \"Key\"),\n",
    "    (\"q\", \"Query\"),\n",
    "    (\"v\", \"Value\"),\n",
    "    (\"z\", \"Mixed Value\"),\n",
    "]:\n",
    "    head_vector_attr_dict[activation_name], head_vector_labels = attr_patch_head_vector(\n",
    "        clean_cache, corrupted_cache, corrupted_grad_cache, activation_name, \"cpu\"\n",
    "    )\n",
    "    sum_head_vector_attr = einops.reduce(\n",
    "        head_vector_attr_dict[activation_name],\n",
    "        \"(layer head) pos -> layer head\",\n",
    "        \"sum\",\n",
    "        layer=model.cfg.n_layers,\n",
    "        head=model.cfg.n_heads,\n",
    "    )\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1f048-e679-4c22-ae27-1e04cfa8461d",
   "metadata": {},
   "source": [
    "# Head Pattern Attribution Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acabfed1-140d-4619-88a4-a1e42e06f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_head_pattern_from_cache(\n",
    "    cache,\n",
    "    device\n",
    ") -> TT[\"layer_and_head_index\", \"batch\", \"dest_pos\", \"src_pos\"]:\n",
    "    \"\"\"Stacks the head patterns from the cache into a single tensor.\"\"\"\n",
    "    stacked_head_pattern = torch.stack(\n",
    "        [cache[\"pattern\", l] for l in range(model.cfg.n_layers)], dim=0\n",
    "    ).to(device)\n",
    "    stacked_head_pattern = einops.rearrange(\n",
    "        stacked_head_pattern,\n",
    "        \"layer batch head_index dest_pos src_pos -> (layer head_index) batch dest_pos src_pos\",\n",
    "    ).to(device)\n",
    "    return stacked_head_pattern\n",
    "\n",
    "\n",
    "def attr_patch_head_pattern(\n",
    "    clean_cache: ActivationCache,\n",
    "    corrupted_cache: ActivationCache,\n",
    "    corrupted_grad_cache: ActivationCache,\n",
    "    device\n",
    ") -> TT[\"component\", \"dest_pos\", \"src_pos\"]:\n",
    "    labels = HEAD_NAMES\n",
    "\n",
    "    clean_head_pattern = stack_head_pattern_from_cache(clean_cache, \"cpu\").to(device)\n",
    "    corrupted_head_pattern = stack_head_pattern_from_cache(corrupted_cache, \"cpu\").to(device)\n",
    "    corrupted_grad_head_pattern = stack_head_pattern_from_cache(corrupted_grad_cache, \"cpu\").to(device)\n",
    "    head_pattern_attr = einops.reduce(\n",
    "        corrupted_grad_head_pattern * (clean_head_pattern - corrupted_head_pattern),\n",
    "        \"component batch dest_pos src_pos -> component dest_pos src_pos\",\n",
    "        \"sum\",\n",
    "    ).to(device)\n",
    "    return head_pattern_attr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2fc558f7-dc81-419f-bc45-b1aeb2da85e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 26044.60 MB  Reserved: 26090.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 26045.30 MB  Reserved: 26078.00 MB\n"
     ]
    }
   ],
   "source": [
    "head_pattern_attr, labels = attr_patch_head_pattern(\n",
    "    clean_cache, corrupted_cache, corrupted_grad_cache, \"cpu\"\n",
    ")\n",
    "\n",
    "head_pattern_attr = einops.rearrange(\n",
    "        head_pattern_attr,\n",
    "        \"(layer head) dest src -> layer head dest src\",\n",
    "        layer=model.cfg.n_layers,\n",
    "        head=model.cfg.n_heads,\n",
    "    )\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d05db8-d81f-463a-99ef-f5e5c2fc7320",
   "metadata": {},
   "source": [
    "# Head Path Attribution Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3737193-1d72-4242-b9d0-adbfa2ba8393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_vector_grad_input_from_grad_cache(\n",
    "    grad_cache: ActivationCache, activation_name: Literal[\"q\", \"k\", \"v\"], layer: int, device\n",
    ") -> TT[\"batch\", \"pos\", \"head_index\", \"d_model\"]:\n",
    "    vector_grad = grad_cache[activation_name, layer].to(device)\n",
    "    ln_scales = grad_cache[\"scale\", layer, \"ln1\"].to(device)\n",
    "    attn_layer_object = model.blocks[layer].attn\n",
    "    if activation_name == \"q\":\n",
    "        W = attn_layer_object.W_Q.to(device)\n",
    "    elif activation_name == \"k\":\n",
    "        W = attn_layer_object.W_K.to(device)\n",
    "    elif activation_name == \"v\":\n",
    "        W = attn_layer_object.W_V.to(device)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid activation name\")\n",
    "\n",
    "    # Original notebook used (batch pos) for second input but that seems to be wrong - double check this computation\n",
    "    return einsum(\n",
    "        \"batch pos head_index d_head, batch pos head_index, head_index d_model d_head -> batch pos head_index d_model\",\n",
    "        vector_grad,\n",
    "        ln_scales.squeeze(-1),\n",
    "        W,\n",
    "    )\n",
    "\n",
    "def get_stacked_head_vector_grad_input(\n",
    "    grad_cache, activation_name: Literal[\"q\", \"k\", \"v\"], device\n",
    ") -> TT[\"layer\", \"batch\", \"pos\", \"head_index\", \"d_model\"]:\n",
    "    return torch.stack(\n",
    "        [\n",
    "            get_head_vector_grad_input_from_grad_cache(grad_cache, activation_name, l, \"cpu\")\n",
    "            for l in range(model.cfg.n_layers)\n",
    "        ],\n",
    "        dim=0,\n",
    "    ).to(device)\n",
    "\n",
    "def get_full_vector_grad_input(\n",
    "    grad_cache, device\n",
    ") -> TT[\"qkv\", \"layer\", \"batch\", \"pos\", \"head_index\", \"d_model\"]:\n",
    "    return torch.stack([get_stacked_head_vector_grad_input(grad_cache, activation_name, \"cpu\").to(device) for activation_name in [\"q\", \"k\", \"v\"]], dim=0).to(device)\n",
    "\n",
    "\n",
    "def attr_patch_head_path(\n",
    "    clean_cache: ActivationCache,\n",
    "    corrupted_cache: ActivationCache,\n",
    "    corrupted_grad_cache: ActivationCache,\n",
    "    device\n",
    ") -> TT[\"qkv\", \"dest_component\", \"src_component\", \"pos\"]:\n",
    "    \"\"\"\n",
    "    Computes the attribution patch along the path between each pair of heads.\n",
    "\n",
    "    Sets this to zero for the path from any late head to any early head\n",
    "\n",
    "    \"\"\"\n",
    "    start_labels = HEAD_NAMES\n",
    "    end_labels = HEAD_NAMES_QKV\n",
    "    full_vector_grad_input = get_full_vector_grad_input(corrupted_grad_cache, \"cpu\")\n",
    "    clean_head_result_stack = clean_cache.stack_head_results(-1)\n",
    "    corrupted_head_result_stack = corrupted_cache.stack_head_results(-1)\n",
    "    diff_head_result = einops.rearrange(\n",
    "        clean_head_result_stack - corrupted_head_result_stack,\n",
    "        \"(layer head_index) batch pos d_model -> layer batch pos head_index d_model\",\n",
    "        layer=model.cfg.n_layers,\n",
    "        head_index=model.cfg.n_heads,\n",
    "    )\n",
    "    path_attr = einsum(\n",
    "        \"qkv layer_end batch pos head_end d_model, layer_start batch pos head_start d_model -> qkv layer_end head_end layer_start head_start pos\",\n",
    "        full_vector_grad_input,\n",
    "        diff_head_result,\n",
    "    )\n",
    "    correct_layer_order_mask = (\n",
    "        torch.arange(model.cfg.n_layers)[None, :, None, None, None, None]\n",
    "        > torch.arange(model.cfg.n_layers)[None, None, None, :, None, None]\n",
    "    ).to(path_attr.device)\n",
    "    zero = torch.zeros(1, device=path_attr.device)\n",
    "    path_attr = torch.where(correct_layer_order_mask, path_attr, zero)\n",
    "\n",
    "    path_attr = einops.rearrange(\n",
    "        path_attr,\n",
    "        \"qkv layer_end head_end layer_start head_start pos -> (layer_end head_end qkv) (layer_start head_start) pos\",\n",
    "    )\n",
    "    return path_attr, end_labels, start_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fee61b64-e942-4159-ad33-1dee88246c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_path_attr, end_labels, start_labels = attr_patch_head_path(\n",
    "    clean_cache, corrupted_cache, corrupted_grad_cache, \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dca5f72c-931c-4fe6-94c0-33d963d87966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 26044.60 MB  Reserved: 26090.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 26045.30 MB  Reserved: 26078.00 MB\n"
     ]
    }
   ],
   "source": [
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ef4d5c1-7ee8-4e06-893b-05c17b4a2c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 26044.60 MB  Reserved: 26090.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 26045.30 MB  Reserved: 26078.00 MB\n"
     ]
    }
   ],
   "source": [
    "head_out_values, head_out_indices = head_out_attr.sum(-1).abs().sort(descending=True)\n",
    "top_head_indices = head_out_indices[:22].sort().values\n",
    "top_end_indices = []\n",
    "top_end_labels = []\n",
    "top_start_indices = []\n",
    "top_start_labels = []\n",
    "\n",
    "for i in top_head_indices:\n",
    "    i = i.item()\n",
    "    top_start_indices.append(i)\n",
    "    top_start_labels.append(start_labels[i])\n",
    "    for j in range(3):\n",
    "        top_end_indices.append(3 * i + j)\n",
    "        top_end_labels.append(end_labels[3 * i + j])\n",
    "        \n",
    "top_head_path_attr = einops.rearrange(\n",
    "    head_path_attr[top_end_indices, :][:, top_start_indices].sum(-1),\n",
    "    \"(head_end qkv) head_start -> qkv head_end head_start\",\n",
    "    qkv=3,\n",
    ")\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "083e15df-c306-4f9f-a3a6-52c5b79b9a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 26044.60 MB  Reserved: 26090.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 26045.30 MB  Reserved: 26078.00 MB\n"
     ]
    }
   ],
   "source": [
    "interesting_heads = [\n",
    "    5 * model.cfg.n_heads + 5,\n",
    "    8 * model.cfg.n_heads + 6,\n",
    "    9 * model.cfg.n_heads + 9,\n",
    "]\n",
    "interesting_head_labels = [HEAD_NAMES[i] for i in interesting_heads]\n",
    "for head_index, label in zip(interesting_heads, interesting_head_labels):\n",
    "    in_paths = head_path_attr[3 * head_index : 3 * head_index + 3].sum(-1)\n",
    "    out_paths = head_path_attr[:, head_index].sum(-1)\n",
    "    out_paths = einops.rearrange(out_paths, \"(layer_head qkv) -> qkv layer_head\", qkv=3)\n",
    "    all_paths = torch.cat([in_paths, out_paths], dim=0)\n",
    "    all_paths = einops.rearrange(\n",
    "        all_paths,\n",
    "        \"path_type (layer head) -> path_type layer head\",\n",
    "        layer=model.cfg.n_layers,\n",
    "        head=model.cfg.n_heads,\n",
    "    )\n",
    "    # TODO - implement visualization for input/output paths per head\n",
    "    \n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3f80b-11fd-4397-bf72-701d7423c7a4",
   "metadata": {},
   "source": [
    "# Validating Attribution vs Activation Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab4327d3-9363-4ea6-a638-99ad86a6e446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 26044.60 MB  Reserved: 26090.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 26045.30 MB  Reserved: 26078.00 MB\n"
     ]
    }
   ],
   "source": [
    "attribution_cache_dict = {}\n",
    "for key in corrupted_grad_cache.cache_dict.keys():\n",
    "    attribution_cache_dict[key] = corrupted_grad_cache.cache_dict[key] * (\n",
    "        clean_cache.cache_dict[key] - corrupted_cache.cache_dict[key]\n",
    "    ).to(\"cpu\")\n",
    "attr_cache = ActivationCache(attribution_cache_dict, model).to(\"cpu\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c1862-c4fb-4dac-8e57-7bd52af0bc3f",
   "metadata": {},
   "source": [
    "# Activation Patching per Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db84d526-4460-400e-8b22-58038cd14822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07e5d55-6d59-4180-ba39-8bd42c52a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_tokens = model.to_str_tokens(clean_tokens[0])\n",
    "context_length = len(str_tokens)\n",
    "every_block_act_patch_result = patching.get_act_patch_block_every(\n",
    "    model, corrupted_tokens, clean_cache.to(device, ioi_metric\n",
    ")  #.to(\"cpu\")\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7295a4fc-e6f1-4053-80ce-7f7c89cc9e3e",
   "metadata": {},
   "source": [
    "# Attribution Patching per Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "94fd352a-df94-416b-8fba-53fd9abe5af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr_patch_block_every(attr_cache, device):\n",
    "    resid_pre_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"resid_pre\"),\n",
    "        \"layer batch pos d_model -> layer pos\",\n",
    "        \"sum\",\n",
    "    ).to(device)\n",
    "    attn_out_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"attn_out\"),\n",
    "        \"layer batch pos d_model -> layer pos\",\n",
    "        \"sum\",\n",
    "    ).to(device)\n",
    "    mlp_out_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"mlp_out\"),\n",
    "        \"layer batch pos d_model -> layer pos\",\n",
    "        \"sum\",\n",
    "    ).to(device)\n",
    "\n",
    "    every_block_attr_patch_result = torch.stack(\n",
    "        [resid_pre_attr, attn_out_attr, mlp_out_attr], dim=0\n",
    "    )\n",
    "    return every_block_attr_patch_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6dce6a26-cc47-451a-bc75-6fabfead62e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 81052.38 MB  Allocated: 26044.60 MB  Reserved: 26090.00 MB\n",
      "GPU 1: 81052.38 MB  Allocated: 26045.30 MB  Reserved: 26078.00 MB\n"
     ]
    }
   ],
   "source": [
    "every_block_attr_patch_result = get_attr_patch_block_every(attr_cache, \"cpu\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ba8592-de04-44ee-955f-0eee971add5d",
   "metadata": {},
   "source": [
    "# Activation Patching per Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ff676-086e-445a-829b-9792b1c7e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "every_head_all_pos_act_patch_result = patching.get_act_patch_attn_head_all_pos_every(\n",
    "    model, corrupted_tokens, clean_cache, ioi_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d4e0ef-44cb-4755-b609-895b430f3dc1",
   "metadata": {},
   "source": [
    "# Attribution Patching per Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c36cfca3-c2ef-4dce-9f75-ee4a874fdc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr_patch_attn_head_all_pos_every(attr_cache, device):\n",
    "    head_out_all_pos_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"z\"),\n",
    "        \"layer batch pos head_index d_head -> layer head_index\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    head_q_all_pos_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"q\"),\n",
    "        \"layer batch pos head_index d_head -> layer head_index\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    head_k_all_pos_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"k\"),\n",
    "        \"layer batch pos head_index d_head -> layer head_index\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    head_v_all_pos_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"v\"),\n",
    "        \"layer batch pos head_index d_head -> layer head_index\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    head_pattern_all_pos_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"pattern\"),\n",
    "        \"layer batch head_index dest_pos src_pos -> layer head_index\",\n",
    "        \"sum\",\n",
    "    )\n",
    "\n",
    "    return torch.stack(\n",
    "        [\n",
    "            head_out_all_pos_attr,\n",
    "            head_q_all_pos_attr,\n",
    "            head_k_all_pos_attr,\n",
    "            head_v_all_pos_attr,\n",
    "            head_pattern_all_pos_attr,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "every_head_all_pos_attr_patch_result = get_attr_patch_attn_head_all_pos_every(\n",
    "    attr_cache, \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cdc2cd-c7d5-48cc-81ac-26fa378bfce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "every_head_by_pos_act_patch_result = patching.get_act_patch_attn_head_by_pos_every(\n",
    "    model, corrupted_tokens, clean_cache, ioi_metric\n",
    ")\n",
    "every_head_by_pos_act_patch_result = einops.rearrange(\n",
    "    every_head_by_pos_act_patch_result,\n",
    "    \"act_type layer pos head -> act_type (layer head) pos\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-finetuning]",
   "language": "python",
   "name": "conda-env-.conda-finetuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
