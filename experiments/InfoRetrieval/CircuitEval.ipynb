{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1560751b-5cdb-480c-8c59-30443892c6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jnainani_umass_edu/codellm/MechInterpCodeLLMs\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jnainani_umass_edu/codellm/MechInterpCodeLLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df76b826-c0b5-4867-9e89-080de3571dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "Primary device: cuda:0\n",
      "GPU 0: 81050.62 MB  Allocated: 0.00 MB  Reserved: 0.00 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520d9f0fdff84597937153ddd7d24207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model CodeLlama-7b-hf into HookedTransformer\n",
      "Primary device: cuda:0\n",
      "GPU 0: 81050.62 MB  Allocated: 26348.48 MB  Reserved: 26370.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from scipy.stats import linregress\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "from typing import List, Union, Optional, Callable\n",
    "from typing_extensions import Literal\n",
    "from functools import partial\n",
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "\n",
    "# THIS IS A LOCAL (MODIFIED) VERSION OF TRANSFORMER_LENS - UNINSTALL PIP/CONDA VERSION BEFORE USE!\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "import transformer_lens.patching as patching\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "\n",
    "import os\n",
    "from config import HF_TOKEN, HF_PATH\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = HF_PATH\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = HF_PATH\n",
    "os.environ[\"HF_HOME\"] = HF_PATH\n",
    "\n",
    "# When using multiple GPUs we use GPU 0 as the primary and switch to the next when it is 90% full\n",
    "num_gpus = torch.cuda.device_count()\n",
    "device_id = 0\n",
    "if num_gpus > 0:\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "def check_gpu_memory(max_alloc=0.9):\n",
    "    if not torch.cuda.is_available():\n",
    "        return\n",
    "    global device_id, device\n",
    "    print(\"Primary device:\", device)\n",
    "    torch.cuda.empty_cache()\n",
    "    max_alloc = 1 if max_alloc > 1 else max_alloc\n",
    "    for gpu in range(num_gpus):\n",
    "        memory_reserved = torch.cuda.memory_reserved(device=gpu)\n",
    "        memory_allocated = torch.cuda.memory_allocated(device=gpu)\n",
    "        total_memory = torch.cuda.get_device_properties(gpu).total_memory \n",
    "        print(f\"GPU {gpu}: {total_memory / (1024**2):.2f} MB  Allocated: {memory_allocated / (1024**2):.2f} MB  Reserved: {memory_reserved / (1024**2):.2f} MB\")\n",
    "                \n",
    "        # Check if the current GPU is getting too full, and if so we switch the primary device to the next GPU\n",
    "        if memory_reserved > max_alloc * total_memory:\n",
    "            if device_id < num_gpus - 1:\n",
    "                device_id += 1\n",
    "                device = f\"cuda:{device_id}\"\n",
    "                print(f\"Switching primary device to {device}\")\n",
    "            else:\n",
    "                print(\"Cannot switch primary device, all GPUs are nearly full\")\n",
    "\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "check_gpu_memory()\n",
    "\n",
    "def timeit(func):\n",
    "    \"\"\"Decorator to measure the execution time of a function.\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Function {func.__name__!r} executed in {end_time - start_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Create transformer\n",
    "model = HookedTransformer.from_pretrained(\"CodeLlama-7b-hf\", n_devices=num_gpus)\n",
    "\n",
    "# We need these so that individual attention heads and MLP inputs can be edited\n",
    "model.set_use_attn_in(True)\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_attn_result(True) # Documentation says this easily burns through GPU memory\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a3c9b2-458d-4e4a-8043-26d714143b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts: [\" D = 'rose'  J = 'document'  A = 'ring'  E = 'cup'  The name of the key that has the value 'rose' is \", \" D = 'rose'  J = 'document'  A = 'ring'  E = 'cup'  The name of the key that has the value 'document' is \", \" D = 'rose'  J = 'document'  A = 'ring'  E = 'cup'  The name of the key that has the value 'ring' is \", \" D = 'rose'  J = 'document'  A = 'ring'  E = 'cup'  The name of the key that has the value 'cup' is \", \" H = 'clock'  G = 'disk'  J = 'hat'  E = 'key'  The name of the key that has the value 'clock' is \", \" H = 'clock'  G = 'disk'  J = 'hat'  E = 'key'  The name of the key that has the value 'disk' is \", \" H = 'clock'  G = 'disk'  J = 'hat'  E = 'key'  The name of the key that has the value 'hat' is \", \" H = 'clock'  G = 'disk'  J = 'hat'  E = 'key'  The name of the key that has the value 'key' is \", \" K = 'sheet'  B = 'bell'  F = 'game'  D = 'boot'  The name of the key that has the value 'sheet' is \", \" K = 'sheet'  B = 'bell'  F = 'game'  D = 'boot'  The name of the key that has the value 'bell' is \", \" K = 'sheet'  B = 'bell'  F = 'game'  D = 'boot'  The name of the key that has the value 'game' is \", \" K = 'sheet'  B = 'bell'  F = 'game'  D = 'boot'  The name of the key that has the value 'boot' is \", \" B = 'watch'  H = 'bag'  G = 'machine'  C = 'glass'  The name of the key that has the value 'watch' is \", \" B = 'watch'  H = 'bag'  G = 'machine'  C = 'glass'  The name of the key that has the value 'bag' is \", \" B = 'watch'  H = 'bag'  G = 'machine'  C = 'glass'  The name of the key that has the value 'machine' is \", \" B = 'watch'  H = 'bag'  G = 'machine'  C = 'glass'  The name of the key that has the value 'glass' is \", \" C = 'car'  B = 'rose'  A = 'cup'  E = 'sheet'  The name of the key that has the value 'car' is \", \" C = 'car'  B = 'rose'  A = 'cup'  E = 'sheet'  The name of the key that has the value 'rose' is \", \" C = 'car'  B = 'rose'  A = 'cup'  E = 'sheet'  The name of the key that has the value 'cup' is \", \" C = 'car'  B = 'rose'  A = 'cup'  E = 'sheet'  The name of the key that has the value 'sheet' is \", \" I = 'boot'  H = 'ring'  C = 'phone'  A = 'leaf'  The name of the key that has the value 'boot' is \", \" I = 'boot'  H = 'ring'  C = 'phone'  A = 'leaf'  The name of the key that has the value 'ring' is \", \" I = 'boot'  H = 'ring'  C = 'phone'  A = 'leaf'  The name of the key that has the value 'phone' is \", \" I = 'boot'  H = 'ring'  C = 'phone'  A = 'leaf'  The name of the key that has the value 'leaf' is \"]\n",
      "Answers: [('D', 'J'), ('J', 'D'), ('A', 'E'), ('E', 'A'), ('H', 'G'), ('G', 'H'), ('J', 'E'), ('E', 'J'), ('K', 'B'), ('B', 'K'), ('F', 'D'), ('D', 'F'), ('B', 'H'), ('H', 'B'), ('G', 'C'), ('C', 'G'), ('C', 'B'), ('B', 'C'), ('A', 'E'), ('E', 'A'), ('I', 'H'), ('H', 'I'), ('C', 'A'), ('A', 'C')]\n",
      "Prompts: 24\n",
      "Answers: 24\n",
      "Clean string 0 <s>  D = 'rose'  J = 'document'  A = 'ring'  E = 'cup'  The name of the key that has the value 'rose' is \n",
      "Corrupted string 0 <s>  D = 'rose'  J = 'document'  A = 'ring'  E = 'cup'  The name of the key that has the value 'document' is \n",
      "Primary device: cuda:0\n",
      "GPU 0: 81050.62 MB  Allocated: 26348.50 MB  Reserved: 26370.00 MB\n",
      "Answer token indices tensor([[360, 435],\n",
      "        [435, 360],\n",
      "        [319, 382],\n",
      "        [382, 319],\n",
      "        [379, 402],\n",
      "        [402, 379],\n",
      "        [435, 382],\n",
      "        [382, 435],\n",
      "        [476, 350],\n",
      "        [350, 476],\n",
      "        [383, 360],\n",
      "        [360, 383],\n",
      "        [350, 379],\n",
      "        [379, 350],\n",
      "        [402, 315],\n",
      "        [315, 402],\n",
      "        [315, 350],\n",
      "        [350, 315],\n",
      "        [319, 382],\n",
      "        [382, 319],\n",
      "        [306, 379],\n",
      "        [379, 306],\n",
      "        [315, 319],\n",
      "        [319, 315]], device='cuda:0')\n",
      "Primary device: cuda:0\n",
      "GPU 0: 81050.62 MB  Allocated: 26348.50 MB  Reserved: 26370.00 MB\n"
     ]
    }
   ],
   "source": [
    "with open('data/info_retrieval/instructed_trial3.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Take the first few dictionaries (e.g., first 3)\n",
    "subset = data[:150]\n",
    "\n",
    "# Initialize lists\n",
    "prompts = []\n",
    "answers = []\n",
    "\n",
    "# Extract prompts and outputs\n",
    "outputs = []\n",
    "for ind, item in enumerate(subset):\n",
    "    # if ind in [0, 1, 3, 4, 5, 6, 7, 9]:\n",
    "    # print(()\n",
    "    if model.to_tokens(item[\"prompt\"]).shape[-1] == 40:\n",
    "        prompts.append(item[\"prompt\"])\n",
    "        outputs.append(item[\"output\"])\n",
    "\n",
    "# Group pairs of outputs and reverse the tuples alternately\n",
    "for i in range(0, len(outputs) - 1, 2):\n",
    "    answers.append((outputs[i], outputs[i + 1]))\n",
    "    answers.append((outputs[i + 1], outputs[i]))\n",
    "\n",
    "# Display the results\n",
    "print(\"Prompts:\", prompts)\n",
    "print(\"Answers:\", answers)\n",
    "print(\"Prompts:\", len(prompts))\n",
    "print(\"Answers:\", len(answers))\n",
    " \n",
    "no_of_prompts = len(prompts)\n",
    "clean_tokens = model.to_tokens(prompts)\n",
    "# Swap each adjacent pair, with a hacky list comprehension\n",
    "corrupted_tokens = clean_tokens[\n",
    "    [(i+1 if i%2==0 else i-1) for i in range(len(clean_tokens)) ]\n",
    "    ]\n",
    "print(\"Clean string 0\", model.to_string(clean_tokens[0]))\n",
    "print(\"Corrupted string 0\", model.to_string(corrupted_tokens[0]))\n",
    "check_gpu_memory()\n",
    "\n",
    "answer_token_indices = torch.tensor([[model.to_single_token(answers[i][j]) for j in range(2)] for i in range(len(answers))], device=device)\n",
    "print(\"Answer token indices\", answer_token_indices)\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca053bd3-2db4-4db8-91de-c4d015cb8401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n",
      "Primary device: cuda:0\n",
      "GPU 0: 81050.62 MB  Allocated: 0.02 MB  Reserved: 2.00 MB\n"
     ]
    }
   ],
   "source": [
    "_, clean_cache = model.to(\"cpu\").run_with_cache(clean_tokens)\n",
    "# clean_cache = clean_cache.to(\"cpu\")\n",
    "check_gpu_memory()\n",
    "\n",
    "# # Process the first prompt\n",
    "# answer_token_indices_first = answer_token_indices[0:1]\n",
    "# clean_value, clean_cache_first, clean_grad_cache_first = get_cache_fwd_and_bwd(model, clean_tokens[0:1], ioi_metric, answer_token_indices_first)\n",
    "# clean_cache_first = clean_cache_first.to('cpu')\n",
    "# clean_grad_cache_first = clean_grad_cache_first.to('cpu')\n",
    "# check_gpu_memory()\n",
    "\n",
    "# delete_variable('clean_value')\n",
    "# get_memory_usage()\n",
    "# # delete_variable('clean_cache_first')\n",
    "# # delete_variable('clean_grad_cache_first')\n",
    "\n",
    "# corrupted_value, corrupted_cache_first, corrupted_grad_cache_first = get_cache_fwd_and_bwd(model, corrupted_tokens[0:1], ioi_metric, answer_token_indices_first)\n",
    "# corrupted_cache_first = corrupted_cache_first.to('cpu')\n",
    "# corrupted_grad_cache_first = corrupted_grad_cache_first.to('cpu')\n",
    "# check_gpu_memory()\n",
    "\n",
    "# delete_variable('corrupted_value')\n",
    "# # delete_variable('corrupted_cache_first')a\n",
    "# # delete_variable('corrupted_grad_cache_first')\n",
    "\n",
    "# for i in range(1, len(clean_tokens)):\n",
    "#     single_clean_tokens = clean_tokens[i:i+1]\n",
    "\n",
    "\n",
    "#     clean_value, clean_cache, clean_grad_cache = model.run_with_cah(model, single_clean_tokens, ioi_metric, single_answer_token_indices)\n",
    "#     clean_cache = clean_cache.to('cpu')\n",
    "#     clean_cache_first = clean_cache_first.concatenate(clean_cache)\n",
    "#     check_gpu_memory()\n",
    "\n",
    "#     delete_variable('clean_cache')\n",
    "#     get_memory_usage()\n",
    "\n",
    "#     print(\"CURRENT INDEX: \", i)\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Test if shapes worked\n",
    "# print(\"cache shape: ,\", clean_cache_first[\"hook_embed\"].shape)\n",
    "# check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc1a35eb-5794-414d-bc0d-ab9ebb6365a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indices': [[1200, 358], [1200, 366], [1211, 366], [1224, 358], [1225, 358], [1225, 366], [1226, 366], [1416, 358], [1416, 366], [1416, 400], [1416, 432], [1580, 485], [1632, 432], [1632, 485], [1632, 517], [1632, 532], [1671, 485], [1671, 517], [1698, 485], [1758, 432], [1758, 485], [1758, 517], [1818, 432], [1818, 448], [1818, 472], [1818, 475], [1818, 482], [1818, 485], [1818, 490], [1818, 494], [1818, 515], [1818, 517], [1818, 526], [1818, 532], [1818, 543], [1818, 544], [1818, 552], [1818, 557], [1818, 562], [1818, 572], [1869, 606], [1950, 485], [1950, 517]], 'labels': [['L12H16Q', 'L11H6'], ['L12H16Q', 'L11H14'], ['L12H19V', 'L11H14'], ['L12H24Q', 'L11H6'], ['L12H24K', 'L11H6'], ['L12H24K', 'L11H14'], ['L12H24V', 'L11H14'], ['L14H24Q', 'L11H6'], ['L14H24Q', 'L11H14'], ['L14H24Q', 'L12H16'], ['L14H24Q', 'L13H16'], ['L16H14V', 'L15H5'], ['L17H0Q', 'L13H16'], ['L17H0Q', 'L15H5'], ['L17H0Q', 'L16H5'], ['L17H0Q', 'L16H20'], ['L17H13Q', 'L15H5'], ['L17H13Q', 'L16H5'], ['L17H22Q', 'L15H5'], ['L18H10Q', 'L13H16'], ['L18H10Q', 'L15H5'], ['L18H10Q', 'L16H5'], ['L18H30Q', 'L13H16'], ['L18H30Q', 'L14H0'], ['L18H30Q', 'L14H24'], ['L18H30Q', 'L14H27'], ['L18H30Q', 'L15H2'], ['L18H30Q', 'L15H5'], ['L18H30Q', 'L15H10'], ['L18H30Q', 'L15H14'], ['L18H30Q', 'L16H3'], ['L18H30Q', 'L16H5'], ['L18H30Q', 'L16H14'], ['L18H30Q', 'L16H20'], ['L18H30Q', 'L16H31'], ['L18H30Q', 'L17H0'], ['L18H30Q', 'L17H8'], ['L18H30Q', 'L17H13'], ['L18H30Q', 'L17H18'], ['L18H30Q', 'L17H28'], ['L19H15Q', 'L18H30'], ['L20H10Q', 'L15H5'], ['L20H10Q', 'L16H5']]}\n"
     ]
    }
   ],
   "source": [
    "# Specify the file name\n",
    "file_name = 'circuits/codellama/infoRet_30prompts_data3.json'\n",
    "\n",
    "# Read the dictionary from the JSON file\n",
    "with open(file_name, 'r') as json_file:\n",
    "    loaded_circuit_dictionary = json.load(json_file)\n",
    "\n",
    "# Print the loaded dictionary to verify its contents\n",
    "print(loaded_circuit_dictionary)\n",
    "\n",
    "keys_comps = []\n",
    "for i in range(len(list(loaded_circuit_dictionary['labels']))):\n",
    "    ind = list(loaded_circuit_dictionary['indices'])[i][1]\n",
    "    layer_ind = ind//32\n",
    "    head_ind = ind%32\n",
    "    # key = f'blocks.{layer_ind}.attn.hook_result'\n",
    "    keys_comps.append((layer_ind, head_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ce0f45c-e2b7-4030-a019-0345453e458d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 81050.62 MB  Allocated: 0.02 MB  Reserved: 2.00 MB\n"
     ]
    }
   ],
   "source": [
    "# get mean activations \n",
    "mean_activations = {}\n",
    "for l in range(model.cfg.n_layers):\n",
    "    key = f'blocks.{l}.attn.hook_result'\n",
    "    mean_activations[l] = torch.sum(clean_cache[key], dim=0).to(\"cpu\")/no_of_prompts #.shape\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1aa29bf-b84c-4f32-a6d6-51c9da6f2fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Primary device: cuda:0\n",
      "GPU 0: 81050.62 MB  Allocated: 0.02 MB  Reserved: 2.00 MB\n",
      "Moving model to device:  cpu\n",
      "tensor([[[ 1.4768e+00,  5.1630e+00, -5.5182e-01,  ...,  2.6307e+00,\n",
      "           2.3592e+00,  2.3185e+00],\n",
      "         [ 1.3779e+00,  6.1556e-01,  2.1970e+00,  ...,  1.7838e+00,\n",
      "           1.0380e+00,  1.4624e+00],\n",
      "         [ 2.2206e-01,  1.1108e+00,  1.2202e+00,  ...,  1.7956e+00,\n",
      "          -6.2309e-01, -3.4731e-01],\n",
      "         ...,\n",
      "         [-6.3867e-01, -2.5225e+00, -1.2109e+00,  ...,  1.2411e+00,\n",
      "          -2.5563e-01,  9.3845e-01],\n",
      "         [-1.3879e+00,  2.6864e-01, -6.1564e-01,  ...,  1.0273e-01,\n",
      "          -7.0560e-01,  7.6431e-01],\n",
      "         [-4.4885e-01,  9.2017e-03, -6.6585e-01,  ..., -2.4200e-02,\n",
      "          -4.9813e-01, -3.7237e-01]],\n",
      "\n",
      "        [[ 1.4768e+00,  5.1630e+00, -5.5182e-01,  ...,  2.6307e+00,\n",
      "           2.3592e+00,  2.3185e+00],\n",
      "         [ 1.3779e+00,  6.1556e-01,  2.1970e+00,  ...,  1.7838e+00,\n",
      "           1.0380e+00,  1.4624e+00],\n",
      "         [ 2.2206e-01,  1.1108e+00,  1.2202e+00,  ...,  1.7956e+00,\n",
      "          -6.2309e-01, -3.4731e-01],\n",
      "         ...,\n",
      "         [-6.5351e-01, -2.5540e+00, -1.2588e+00,  ...,  1.2471e+00,\n",
      "          -2.3979e-01,  9.0563e-01],\n",
      "         [-1.3272e+00,  2.5785e-01, -5.8178e-01,  ...,  1.3785e-01,\n",
      "          -6.9224e-01,  7.3607e-01],\n",
      "         [-3.6109e-01, -6.1026e-02, -6.6454e-01,  ..., -6.1267e-02,\n",
      "          -4.5875e-01, -4.1716e-01]],\n",
      "\n",
      "        [[ 1.4768e+00,  5.1630e+00, -5.5182e-01,  ...,  2.6307e+00,\n",
      "           2.3592e+00,  2.3185e+00],\n",
      "         [ 1.3779e+00,  6.1556e-01,  2.1970e+00,  ...,  1.7838e+00,\n",
      "           1.0380e+00,  1.4624e+00],\n",
      "         [ 2.2206e-01,  1.1108e+00,  1.2202e+00,  ...,  1.7956e+00,\n",
      "          -6.2309e-01, -3.4731e-01],\n",
      "         ...,\n",
      "         [-6.3011e-01, -2.5370e+00, -1.2176e+00,  ...,  1.2646e+00,\n",
      "          -2.3932e-01,  9.3917e-01],\n",
      "         [-1.3682e+00,  2.4581e-01, -6.0591e-01,  ...,  1.0718e-01,\n",
      "          -7.2149e-01,  7.1372e-01],\n",
      "         [-4.4808e-01, -1.8678e-01, -6.3976e-01,  ..., -9.3919e-02,\n",
      "          -5.4786e-01, -4.6407e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.4768e+00,  5.1630e+00, -5.5182e-01,  ...,  2.6307e+00,\n",
      "           2.3592e+00,  2.3185e+00],\n",
      "         [ 1.3779e+00,  6.1556e-01,  2.1970e+00,  ...,  1.7838e+00,\n",
      "           1.0380e+00,  1.4624e+00],\n",
      "         [ 5.6366e-01,  1.1830e+00,  1.2221e+00,  ...,  2.0913e+00,\n",
      "          -5.5655e-01, -2.3670e-01],\n",
      "         ...,\n",
      "         [-6.7211e-01, -2.5254e+00, -1.1681e+00,  ...,  1.2562e+00,\n",
      "          -2.3123e-01,  9.0407e-01],\n",
      "         [-1.3644e+00,  3.5867e-01, -5.1897e-01,  ...,  1.0957e-01,\n",
      "          -6.8981e-01,  7.2016e-01],\n",
      "         [-3.7762e-01,  1.2250e-01, -4.6420e-01,  ..., -5.4290e-02,\n",
      "          -4.6703e-01, -3.1428e-01]],\n",
      "\n",
      "        [[ 1.4768e+00,  5.1630e+00, -5.5182e-01,  ...,  2.6307e+00,\n",
      "           2.3592e+00,  2.3185e+00],\n",
      "         [ 1.3779e+00,  6.1556e-01,  2.1970e+00,  ...,  1.7838e+00,\n",
      "           1.0380e+00,  1.4624e+00],\n",
      "         [ 5.6366e-01,  1.1830e+00,  1.2221e+00,  ...,  2.0913e+00,\n",
      "          -5.5655e-01, -2.3670e-01],\n",
      "         ...,\n",
      "         [-6.4997e-01, -2.5232e+00, -1.1228e+00,  ...,  1.2567e+00,\n",
      "          -2.5027e-01,  9.1715e-01],\n",
      "         [-1.3207e+00,  3.7258e-01, -4.9835e-01,  ...,  1.4586e-01,\n",
      "          -7.1821e-01,  7.2362e-01],\n",
      "         [-2.6268e-01,  1.9482e-01, -4.6283e-01,  ..., -4.1429e-03,\n",
      "          -4.6016e-01, -3.6666e-01]],\n",
      "\n",
      "        [[ 1.4768e+00,  5.1630e+00, -5.5182e-01,  ...,  2.6307e+00,\n",
      "           2.3592e+00,  2.3185e+00],\n",
      "         [ 1.3779e+00,  6.1556e-01,  2.1970e+00,  ...,  1.7838e+00,\n",
      "           1.0380e+00,  1.4624e+00],\n",
      "         [ 5.6366e-01,  1.1830e+00,  1.2221e+00,  ...,  2.0913e+00,\n",
      "          -5.5655e-01, -2.3670e-01],\n",
      "         ...,\n",
      "         [-6.6674e-01, -2.5296e+00, -1.1376e+00,  ...,  1.2638e+00,\n",
      "          -2.5950e-01,  9.1703e-01],\n",
      "         [-1.3674e+00,  2.8357e-01, -5.5411e-01,  ...,  1.2474e-01,\n",
      "          -6.9406e-01,  7.2047e-01],\n",
      "         [-3.5235e-01, -8.7625e-02, -6.2532e-01,  ...,  1.2420e-02,\n",
      "          -4.8873e-01, -4.2194e-01]]], grad_fn=<AddBackward0>)\n",
      "Primary device: cuda:0\n",
      "GPU 0: 81050.62 MB  Allocated: 0.02 MB  Reserved: 2.00 MB\n"
     ]
    }
   ],
   "source": [
    "def ablate_setter(corrupted_activation, layer_ind, clean_activation, mean_activation=mean_activations,  keys_comps=keys_comps):\n",
    "    for hed in range(32):\n",
    "        if (layer_ind, hed) not in keys_comps:\n",
    "            mean_activation_broadcasted = mean_activation[layer_ind][:, head_ind, :].unsqueeze(0).expand(no_of_prompts, -1, -1)\n",
    "            # Replace the values at the 3rd dimension of \"hed\" in new_cache[key]\n",
    "            clean_activation[:, :, hed, :] = mean_activation_broadcasted\n",
    "    return clean_activation\n",
    "\n",
    "def ablating_hook(corrupted_activation, layer_ind, hook, clean_activation, mean_activation=mean_activations,  keys_comps=keys_comps):\n",
    "    return ablate_setter(corrupted_activation, layer_ind, clean_activation, mean_activation=mean_activations,  keys_comps=keys_comps)\n",
    "\n",
    "hooks_tuple = []\n",
    "\n",
    "for l in range(model.cfg.n_layers):\n",
    "    key = f'blocks.{l}.attn.hook_result'\n",
    "    \n",
    "    # Make a partial copy of the original cache for the current key\n",
    "    new_cache_temp = clean_cache[key].clone()\n",
    "    \n",
    "    current_hook = partial(\n",
    "    ablating_hook,\n",
    "    layer_ind=l,\n",
    "    clean_activation = new_cache_temp\n",
    "    )   \n",
    "    hooks_tuple.append((key, current_hook))\n",
    "\n",
    "print(type(hooks_tuple))\n",
    "check_gpu_memory()\n",
    "\n",
    "patched_logits = model.to(\"cpu\").run_with_hooks(\n",
    "             clean_tokens, fwd_hooks=hooks_tuple, bwd_hooks=None)\n",
    "print(patched_logits)\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7997834e-33ef-43dd-be00-73db9d192be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 40, 32016])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patched_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4d29a5b-2b73-4748-94ca-6cb7491c39be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  permit\n",
      "Act answer:  D\n",
      "Predicted:  permit\n",
      "Act answer:  J\n",
      "Predicted:  permit\n",
      "Act answer:  A\n",
      "Predicted:  permit\n",
      "Act answer:  E\n",
      "Predicted:  permit\n",
      "Act answer:  H\n",
      "Predicted:  permit\n",
      "Act answer:  G\n",
      "Predicted:  permit\n",
      "Act answer:  J\n",
      "Predicted:  permit\n",
      "Act answer:  E\n",
      "Predicted:  permit\n",
      "Act answer:  K\n",
      "Predicted:  permit\n",
      "Act answer:  B\n",
      "Predicted:  permit\n",
      "Act answer:  F\n",
      "Predicted:  permit\n",
      "Act answer:  D\n",
      "Predicted:  permit\n",
      "Act answer:  B\n",
      "Predicted:  permit\n",
      "Act answer:  H\n",
      "Predicted:  permit\n",
      "Act answer:  G\n",
      "Predicted:  permit\n",
      "Act answer:  C\n",
      "Predicted:  permit\n",
      "Act answer:  C\n",
      "Predicted:  permit\n",
      "Act answer:  B\n",
      "Predicted:  permit\n",
      "Act answer:  A\n",
      "Predicted:  permit\n",
      "Act answer:  E\n",
      "Predicted:  permit\n",
      "Act answer:  I\n",
      "Predicted:  permit\n",
      "Act answer:  H\n",
      "Predicted:  permit\n",
      "Act answer:  C\n",
      "Predicted:  permit\n",
      "Act answer:  A\n"
     ]
    }
   ],
   "source": [
    "model_answers = torch.argmax(patched_logits[:, -1, :], dim=-1)\n",
    "for some_ind in range(len(model_answers)):\n",
    "    print(\"Predicted: \", model.to_string(model_answers[some_ind]))\n",
    "    print(\"Act answer: \", answers[some_ind][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4277e035-3407-49f6-aeed-3304172bb52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('D', 'J'),\n",
       " ('J', 'D'),\n",
       " ('A', 'E'),\n",
       " ('E', 'A'),\n",
       " ('H', 'G'),\n",
       " ('G', 'H'),\n",
       " ('J', 'E'),\n",
       " ('E', 'J'),\n",
       " ('K', 'B'),\n",
       " ('B', 'K'),\n",
       " ('F', 'D'),\n",
       " ('D', 'F'),\n",
       " ('B', 'H'),\n",
       " ('H', 'B'),\n",
       " ('G', 'C'),\n",
       " ('C', 'G'),\n",
       " ('C', 'B'),\n",
       " ('B', 'C'),\n",
       " ('A', 'E'),\n",
       " ('E', 'A'),\n",
       " ('I', 'H'),\n",
       " ('H', 'I'),\n",
       " ('C', 'A'),\n",
       " ('A', 'C')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d08e866a-ec7c-45fb-a941-29b1ea6f8150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfb883f-1c53-42ae-9083-d1db5e4e6471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-finetuning]",
   "language": "python",
   "name": "conda-env-.conda-finetuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
