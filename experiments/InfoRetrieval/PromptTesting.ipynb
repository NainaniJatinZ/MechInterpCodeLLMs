{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4302d05d-77b2-43f1-a32b-74c88e05a8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jnainani_umass_edu/codellm/MechInterpCodeLLMs\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jnainani_umass_edu/codellm/MechInterpCodeLLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1072dfb-680d-4caa-85b6-cf3af9d5aa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jnainani_umass_edu/.conda/envs/finetuning/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "from typing import List, Union, Optional, Callable\n",
    "from typing_extensions import Literal\n",
    "from functools import partial\n",
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "\n",
    "import argparse\n",
    "\n",
    "# THIS IS A LOCAL (MODIFIED) VERSION OF TRANSFORMER_LENS - UNINSTALL PIP/CONDA VERSION BEFORE USE!\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "import transformer_lens.patching as patching\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "\n",
    "import utils_patch.patch_helper_functions as helper\n",
    "\n",
    "import os\n",
    "from config import HF_TOKEN, HF_PATH\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = HF_PATH\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = HF_PATH\n",
    "os.environ[\"HF_HOME\"] = HF_PATH\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "device_id = 0\n",
    "if num_gpus > 0:\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "    \n",
    "def check_gpu_memory(max_alloc=0.9):\n",
    "    if not torch.cuda.is_available():\n",
    "        return\n",
    "    global device_id, device\n",
    "    print(\"Primary device:\", device)\n",
    "    torch.cuda.empty_cache()\n",
    "    max_alloc = 1 if max_alloc > 1 else max_alloc\n",
    "    for gpu in range(num_gpus):\n",
    "        memory_reserved = torch.cuda.memory_reserved(device=gpu)\n",
    "        memory_allocated = torch.cuda.memory_allocated(device=gpu)\n",
    "        total_memory = torch.cuda.get_device_properties(gpu).total_memory \n",
    "        print(f\"GPU {gpu}: {total_memory / (1024**2):.2f} MB  Allocated: {memory_allocated / (1024**2):.2f} MB  Reserved: {memory_reserved / (1024**2):.2f} MB\")\n",
    "                \n",
    "        # Check if the current GPU is getting too full, and if so we switch the primary device to the next GPU\n",
    "        if memory_reserved > max_alloc * total_memory:\n",
    "            if device_id < num_gpus - 1:\n",
    "                device_id += 1\n",
    "                device = f\"cuda:{device_id}\"\n",
    "                print(f\"Switching primary device to {device}\")\n",
    "            else:\n",
    "                print(\"Cannot switch primary device, all GPUs are nearly full\")\n",
    "\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8199f45b-6557-480a-8c5d-eb18e3520e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d1e31aa2ef4fa4b28618f079761aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model CodeLlama-7b-hf into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"CodeLlama-7b-hf\") #, n_devices=num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1142f69-4c0d-4b9a-b91b-3a4232ca8a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e55e2144ee8446cb730df43e339a02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed6b1f9da934bf6b91f9e58c6eb241d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf8422b37a4479abc66e5991c4f3c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30dafa4380b4032815bc286864fea96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bbdd9422124353ae40cbc8d8c2b0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9438defce9b54983aaa59f86e3f940ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3577fa1851e4293b330449f98d9ce00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b774b7ea3ab458094498d17fef3ad14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34fe9cc062641f898b8a5f2e41761de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285c77aacde040f0be84bb592e4cd369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635142886de04754850a9e044847ba02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-2-7b-hf into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac1ab46-ddff-423e-9ac0-cc64c31acea7",
   "metadata": {},
   "source": [
    "# Iterate over a good prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d595459-84be-47da-8f14-7c7bee9551e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aa0f9888-be09-4778-b1ce-bcc879402755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables:  'K' = gift.  'J' = seed.  'I' = map.  'H' = chemical.  The name of the variable that has gift is '\n",
      "K\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('experiments/old_trials/finetuneMI/data/objects.csv')\n",
    "\n",
    "def generate_prompts(num_prompts=40):\n",
    "    # operators = ['+', '-', '*', '**', '%', '<', '>', '<=', '>=', '=='] #['+', '-', '*', '**', '%']\n",
    "    functions = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K']\n",
    "    prompts = []\n",
    "    \n",
    "    for _ in range(num_prompts):\n",
    "        sampled_df = df.sample(n=4)\n",
    "        selected_operators = list(sampled_df.object_name)  #random.sample(operators, 4) #\n",
    "        selected_functions = random.sample(functions, 4)\n",
    "        prompt = \"Variables: \"\n",
    "        # prompt = \"#For this set of variables, return the variable name containing the specified item.\"\n",
    "        \n",
    "        examples = []\n",
    "        op_to_func_mapping = {}\n",
    "\n",
    "        for op, func in zip(selected_operators, selected_functions):\n",
    "            prompt += f\" '{func}' = {op}. \"\n",
    "            # prompt += f\"    return a {op} b\\n\"\n",
    "            # prompt += f\"def {func}(a, b):\\n\"\n",
    "            # prompt += f\"    return a {op} b\\n\"\n",
    "            examples.append([func,op])\n",
    "            op_to_func_mapping[op] = func\n",
    "        \n",
    "        for i in range(4):\n",
    "            chosen_op = selected_operators[i] #random.sample(selected_operators, 1)[0]\n",
    "            prompt2add = prompt+ f\" The name of the variable that has {chosen_op} is '\"\n",
    "            # prompt2add = prompt+f\"Question: What is the name of the variable containing '{chosen_op}'? Answer: \"\n",
    "            prompts.append({\"prompt\": prompt2add, \"output\": op_to_func_mapping[chosen_op]})\n",
    "    \n",
    "    print(prompts[0]['prompt'])\n",
    "    print(prompts[0]['output'])\n",
    "    with open(\"data/info_retrieval/instructed_trial6_llama2.json\", \"w\") as f:\n",
    "        json.dump(prompts, f, indent=4)\n",
    "\n",
    "generate_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b4692065-32e5-4b7a-ae7d-80bedc47af38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation complete. Check the output.json file for results.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Function to transform the prompt using regex\n",
    "def transform_prompt(original_prompt):\n",
    "    # Extract key-value pairs from the original prompt\n",
    "    pairs = re.findall(r\"(\\w) = '([^']+)'\", original_prompt)\n",
    "    \n",
    "    # Extract the target item from the original prompt\n",
    "    target_item = re.search(r\"value '([^']+)'\", original_prompt).group(1)\n",
    "    \n",
    "    # Create a dictionary from the pairs for easier lookup\n",
    "    pairs_dict = {key: value for key, value in pairs}\n",
    "    \n",
    "    # Create the new format for the prompt\n",
    "    # new_prompt = \" \".join([f\"The {value} is in Box {key}.\" for key, value in pairs]) + f\" Name of the Box which has {target_item} is \"\n",
    "    new_prompt = \" \".join([f\"'{key}' has the '{value}'.\" for key, value in pairs]) + f\" The '{target_item}' is in '\"\n",
    "    \n",
    "    return new_prompt\n",
    "\n",
    "# Read the JSON file\n",
    "with open('data/info_retrieval/instructed_trial3.json', 'r') as infile:\n",
    "    data = json.load(infile)\n",
    "\n",
    "# Process each prompt-output pair\n",
    "new_data = []\n",
    "for pair in data:\n",
    "    original_prompt = pair['prompt']\n",
    "    transformed_prompt = transform_prompt(original_prompt)\n",
    "    new_pair = {\"prompt\": transformed_prompt, \"output\": pair['output']}\n",
    "    new_data.append(new_pair)\n",
    "\n",
    "# Write the transformed pairs to a new JSON file\n",
    "with open('data/info_retrieval/instructed_trial5_NL.json', 'w') as outfile:\n",
    "    json.dump(new_data, outfile, indent=4)\n",
    "\n",
    "print(\"Transformation complete. Check the output.json file for results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae94566-c8d9-4330-9cce-c7295132e6ff",
   "metadata": {},
   "source": [
    "# Load and Calculate the number of tokens and prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f5109dc0-ff86-4464-961c-3122e6270253",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/info_retrieval/instructed_trial6_llama2.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Take the first few dictionaries (e.g., first 3)\n",
    "subset = data #[9:]\n",
    "\n",
    "# Initialize lists\n",
    "prompts = []\n",
    "answers = []\n",
    "\n",
    "# Extract prompts and outputs\n",
    "outputs = []\n",
    "for ind, item in enumerate(subset):\n",
    "    # if ind in [0, 1, 3, 4, 5, 6, 7, 9]:\n",
    "    prompts.append(item[\"prompt\"])\n",
    "    outputs.append(item[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "15d1f36d-dd18-4464-ae5e-18bc09982cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Variables:  'K' = gift.  'J' = seed.  'I' = map.  'H' = chemical.  The name of the variable that has gift is '\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "51ead190-3188-4ce7-9535-51a65e09b39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9422d4f1-f540-4d00-83b1-f5d8cc39c4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 43])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.to_tokens(prompts[20])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6d72c074-2f01-49a9-8347-2534b4f3de6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d07cd8b5c5424abc59e192875187ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Variables:  'A' = engine.  'G' = ball.  'J' = book.  'F' = key.  The name of the variable that has book is 'G\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(prompts[10], max_new_tokens=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f19d25a-7f3b-49ad-8af5-01c2af4f06a3",
   "metadata": {},
   "source": [
    "# Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "92a72697-0370-4490-9c0c-28361f47d71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:  K\n",
      "ans:  K\n",
      "pred:  J\n",
      "ans:  J\n",
      "pred:  I\n",
      "ans:  I\n",
      "pred:  H\n",
      "ans:  H\n",
      "pred:  C\n",
      "ans:  C\n",
      "pred:  C\n",
      "pred:  P\n",
      "pred:  M\n",
      "pred:  A\n",
      "ans:  A\n",
      "pred:  G\n",
      "ans:  G\n",
      "pred:  J\n",
      "ans:  J\n",
      "pred:  F\n",
      "ans:  F\n",
      "pred:  I\n",
      "ans:  I\n",
      "pred:  D\n",
      "ans:  D\n",
      "pred:  K\n",
      "ans:  K\n",
      "pred:  A\n",
      "ans:  A\n",
      "pred:  F\n",
      "ans:  F\n",
      "pred:  K\n",
      "ans:  K\n",
      "pred:  G\n",
      "pred:  J\n",
      "ans:  J\n",
      "pred:  E\n",
      "ans:  E\n",
      "pred:  F\n",
      "ans:  F\n",
      "pred:  G\n",
      "ans:  G\n",
      "pred:  C\n",
      "ans:  C\n",
      "pred:  E\n",
      "ans:  E\n",
      "pred:  I\n",
      "ans:  I\n",
      "pred:  A\n",
      "ans:  A\n",
      "pred:  F\n",
      "ans:  F\n",
      "pred:  A\n",
      "ans:  A\n",
      "pred:  G\n",
      "ans:  G\n",
      "pred:  H\n",
      "ans:  H\n",
      "pred:  J\n",
      "ans:  J\n",
      "pred:  E\n",
      "ans:  E\n",
      "pred:  B\n",
      "ans:  B\n",
      "pred:  B\n",
      "pred:  I\n",
      "ans:  I\n",
      "pred:  K\n",
      "ans:  K\n",
      "pred:  C\n",
      "ans:  C\n",
      "pred:  J\n",
      "ans:  J\n",
      "pred:  H\n",
      "ans:  H\n",
      "pred:  J\n",
      "ans:  J\n",
      "pred:  D\n",
      "pred:  C\n",
      "pred:  H\n",
      "ans:  H\n",
      "pred:  E\n",
      "pred:  C\n",
      "ans:  C\n",
      "pred:  M\n",
      "pred:  K\n",
      "ans:  K\n",
      "pred:  D\n",
      "ans:  D\n",
      "pred:  H\n",
      "ans:  H\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_next_token_accuracy(prompts, answers, model):\n",
    "    # total_examples = len(prompts)\n",
    "    correct_predictions = 0\n",
    "    for ind in range(len(prompts)):\n",
    "        # print(prompts[ind])\n",
    "        with torch.no_grad():\n",
    "            clean_tokens = model.to_tokens(prompts[ind])\n",
    "            logits = model(clean_tokens)[:, -1, :]\n",
    "            model_answers = torch.argmax(logits, dim=-1)\n",
    "            print(\"pred: \",model.to_string(model_answers))\n",
    "            # print(\"ans: \",answers[ind])\n",
    "            if model.to_string(model_answers).lower() == answers[ind].lower():\n",
    "                # print(\"pred: \",model.to_string(model_answers))\n",
    "                print(\"ans: \",answers[ind])\n",
    "                correct_predictions+=1\n",
    "    return correct_predictions / len(prompts)\n",
    "\n",
    "evaluate_next_token_accuracy(prompts[:50], outputs[:50], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b289f26c-f5db-4fb7-a276-f5563e51576c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a8f30-d1c2-46bb-800f-00dd944ca630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a0b5a-8659-419f-9696-b47dc369bf7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc0db70-24d1-4219-9a41-6fc513a5a24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ae84f-820c-4685-92dc-e4a3a4bf6337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4d2bdcc0-b4b6-41ae-b50e-6036f9b74f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df23e2d37f14d41a0b99b69b02226f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'lyh = \"-\"\\njnh = \"+\"\\nthj = \"*\"\\nbgd = \"\"\\nthe name of the variable with value \"*\" is 4\\n\\\\end{'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(prompts[0], max_new_tokens=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f244ab20-2b77-4f03-9cee-99c5930233a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([796], device='cuda:0')\n",
      "Z\n",
      "tensor([612], device='cuda:0')\n",
      "Y\n",
      "tensor([29896], device='cuda:0')\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# for prompt in prompts: \n",
    "#     clean_tokens = model.to_tokens(prompt)\n",
    "#     logits = model(clean_tokens)[:, -1, :]\n",
    "#     model_answers = torch.argmax(logits, dim=-1)\n",
    "#     print(model_answers)\n",
    "#     print(model.to_string(model_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "26eed411-c41f-45a6-9c9e-884021ba5356",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/  prompt_dataset_zero_shot.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Take the first few dictionaries (e.g., first 3)\n",
    "subset = data[9:]\n",
    "\n",
    "# Initialize lists\n",
    "prompts = []\n",
    "answers = []\n",
    "\n",
    "# Extract prompts and outputs\n",
    "outputs = []\n",
    "for ind, item in enumerate(subset):\n",
    "    # if ind in [0, 1, 3, 4, 5, 6, 7, 9]:\n",
    "    prompts.append(item[\"prompt\"])\n",
    "    outputs.append(item[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9ff653aa-b94c-4518-93dc-b638ca3d8f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \"-\"\n",
      "Y = \"+\"\n",
      "Z = \"*\"\n",
      "W = \"<\"\n",
      "the name of the variable with value \"*\" is \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bb2b6898-257c-418c-8d3d-3e8c32b0ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_next_token_accuracy(prompts, answers, model):\n",
    "    # total_examples = len(prompts)\n",
    "    correct_predictions = 0\n",
    "    for ind in range(len(prompts)):\n",
    "        # print(prompts[ind])\n",
    "        with torch.no_grad():\n",
    "            clean_tokens = model.to_tokens(prompts[ind])\n",
    "            logits = model(clean_tokens)[:, -1, :]\n",
    "            model_answers = torch.argmax(logits, dim=-1)\n",
    "            print(\"pred: \",model.to_string(model_answers))\n",
    "            if model.to_string(model_answers) == answers[ind]:\n",
    "                # print(\"pred: \",model.to_string(model_answers))\n",
    "                print(\"ans: \",answers[ind])\n",
    "                correct_predictions+=1\n",
    "    return correct_predictions / len(prompts)\n",
    "\n",
    "# evaluate_next_token_accuracy(prompts, answers, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c63ac1d6-16f2-4cbd-98bf-a7716b61b0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:  Z\n",
      "ans:  Z\n",
      "pred:  Y\n",
      "ans:  Y\n",
      "pred:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_next_token_accuracy(prompts, answers, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fbede-ae63-4f66-b0a5-de8a6c0c75f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d2f8cd-8508-4eaa-a1b4-e1b337e88a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5ea67-b3ac-4b5a-ac35-79170b9d8ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c50402c-a9bf-4ac3-9ff9-7b90e369f202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d1d35b-e1ca-4679-8280-8d418d75538d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43a00f-e9f4-4402-85e5-9adb6ccfd939",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import json\n",
    "\n",
    "def generate_prompts(num_prompts=400):\n",
    "    operators = ['+', '-', '*', '**', '%', '<', '>', '<=', '>=', '==']\n",
    "    functions = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K']\n",
    "    prompts = []\n",
    "    \n",
    "    for _ in range(num_prompts):\n",
    "        selected_operators = random.sample(operators, 11)\n",
    "        selected_functions = random.sample(functions, 11)\n",
    "        \n",
    "        prompt = \"For this set of functions, return the function which uses the specified operator.\\n\\n\"\n",
    "        \n",
    "        examples = []\n",
    "        op_to_func_mapping = {}\n",
    "\n",
    "        for op, func in zip(selected_operators, selected_functions):\n",
    "            prompt += f\"def {func}(a, b):\\n\"\n",
    "            prompt += f\"    return a {op} b\\n\"\n",
    "            examples.append([func,op])\n",
    "            op_to_func_mapping[op] = func\n",
    "        \n",
    "\n",
    "        \n",
    "        prompt += f\"\\nExample: the name of the function that uses the '{examples[0][1]}' operator is \"\n",
    "        prompt += examples[0][0]\n",
    "        prompt += f\"\\nExample: the name of the function that uses the '{examples[1][1]}' operator is \"\n",
    "        prompt += examples[1][0]\n",
    "\n",
    "        selected_operators.remove(examples[0][1])\n",
    "        selected_operators.remove(examples[1][1])\n",
    "\n",
    "        chosen_op = random.sample(selected_operators, 1)[0]\n",
    "\n",
    "        prompt += f\"\\nExample: the name of the function that uses the '{chosen_op}' operator is \"\n",
    "\n",
    "        \n",
    "        prompts.append({\"prompt\": prompt, \"output\": op_to_func_mapping[chosen_op]})\n",
    "    \n",
    "    print(prompts[0]['prompt'])\n",
    "    print(prompts[0]['output'])\n",
    "    with open(\"prompts_dataset.json\", \"w\") as f:\n",
    "        json.dump(prompts, f, indent=4)\n",
    "\n",
    "generate_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96ed79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /Users/zephyr/Documents/mechReasonCodeLLM/MechInterpCodeLLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74360c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zephyr/Documents/mechReasonCodeLLM/MechInterpCodeLLMs/experiments/InfoRetrieval\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a48fbd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C= * \n",
      "D= % \n",
      "F= ** \n",
      "J= + \n",
      "G= - \n",
      "The name of the variable that contains the '*' operator is \n",
      "C\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import json\n",
    "\n",
    "def generate_prompts(num_prompts=40):\n",
    "    operators = ['+', '-', '*', '**', '%']\n",
    "    functions = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K']\n",
    "    prompts = []\n",
    "    \n",
    "    for _ in range(num_prompts):\n",
    "\n",
    "        selected_operators = random.sample(operators, 5)\n",
    "        selected_functions = random.sample(functions, 5)\n",
    "        prompt = \"\"\n",
    "        # prompt = \"For this set of variables, return the variable name containing the specified operator.\\n\"\n",
    "        \n",
    "        examples = []\n",
    "        op_to_func_mapping = {}\n",
    "\n",
    "        for op, func in zip(selected_operators, selected_functions):\n",
    "            prompt += f\"{func}= {op} \\n\"\n",
    "            # prompt += f\"    return a {op} b\\n\"\n",
    "            examples.append([func,op])\n",
    "            op_to_func_mapping[op] = func\n",
    "        \n",
    "        for i in range(5):\n",
    "            chosen_op = selected_operators[i] #random.sample(selected_operators, 1)[0]\n",
    "            prompt2add = prompt+ f\"The name of the variable that contains the '{chosen_op}' operator is \"\n",
    "            prompts.append({\"prompt\": prompt2add, \"output\": op_to_func_mapping[chosen_op]})\n",
    "    \n",
    "    print(prompts[0]['prompt'])\n",
    "    print(prompts[0]['output'])\n",
    "    with open(\"../../data/info_retrieval/trial1.json\", \"w\") as f:\n",
    "        json.dump(prompts, f, indent=4)\n",
    "\n",
    "generate_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec742b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "['document', 'pot', 'magnet', 'game', 'cross', 'map']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c332cbf2",
   "metadata": {},
   "source": [
    "For this set of variables, return the variable name containing the specified operator.\n",
    "D = %\n",
    "B = -\n",
    "K = *\n",
    "F = **\n",
    "I = +\n",
    "The name of the variable that contains the '%' operator is \n",
    "D\n",
    "\n",
    "\n",
    "D = %\n",
    "B = -\n",
    "K = *\n",
    "F = **\n",
    "I = +\n",
    "O = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84d1379b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plant', 'leaf', 'magnet', 'key', 'brain']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('../old_trials/finetuneMI/data/objects.csv')\n",
    "\n",
    "# Randomly sample 5 rows from the DataFrame\n",
    "sampled_df = df.sample(n=5)\n",
    "\n",
    "# Display the sampled rows\n",
    "print(list(sampled_df.object_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ecec9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J = ['chemical'] \n",
      "G = ['ball'] \n",
      "D = ['fan'] \n",
      "E = ['brain'] \n",
      "The name of the variable that contains 'chemical' is \n",
      "J\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_prompts(num_prompts=40):\n",
    "    # operators = ['+', '-', '*', '**', '%']\n",
    "    functions = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K']\n",
    "    prompts = []\n",
    "    \n",
    "    for _ in range(num_prompts):\n",
    "        sampled_df = df.sample(n=4)\n",
    "        selected_operators = list(sampled_df.object_name) #random.sample(operators, 5)\n",
    "        selected_functions = random.sample(functions, 4)\n",
    "        prompt = \"\"\n",
    "        # prompt = \"For this set of variables, return the variable name containing the specified operator.\\n\"\n",
    "        \n",
    "        examples = []\n",
    "        op_to_func_mapping = {}\n",
    "\n",
    "        for op, func in zip(selected_operators, selected_functions):\n",
    "            prompt += f\"{func} = ['{op}'] \\n\"\n",
    "            # prompt += f\"    return a {op} b\\n\"\n",
    "            examples.append([func,op])\n",
    "            op_to_func_mapping[op] = func\n",
    "        \n",
    "        for i in range(4):\n",
    "            chosen_op = selected_operators[i] #random.sample(selected_operators, 1)[0]\n",
    "            prompt2add = prompt+ f\"The name of the variable that contains '{chosen_op}' is \"\n",
    "            prompts.append({\"prompt\": prompt2add, \"output\": op_to_func_mapping[chosen_op]})\n",
    "    \n",
    "    print(prompts[0]['prompt'])\n",
    "    print(prompts[0]['output'])\n",
    "    with open(\"../../data/info_retrieval/trial4.json\", \"w\") as f:\n",
    "        json.dump(prompts, f, indent=4)\n",
    "\n",
    "generate_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119d7cd1",
   "metadata": {},
   "source": [
    "J = ['chemical'] \n",
    "G = ['ball'] \n",
    "D = ['fan'] \n",
    "E = ['brain'] \n",
    "\n",
    "## Example from entity tracking paper:\n",
    "move -> Move the chemical from J to E.\n",
    "## List operation version\n",
    "E.append(J)\n",
    "J.pop()\n",
    "\n",
    "## Example from entity tracking paper:\n",
    "remove -> Remove the fan from D.\n",
    "## List operation version\n",
    "D.pop()\n",
    "\n",
    "## Example from entity tracking paper:\n",
    "put -> Put the car into G.\n",
    "## List operation version\n",
    "G.append('car')\n",
    "\n",
    "\n",
    "## Example queries: \n",
    "\n",
    "1. The name of the variable that contains 'chemical' is\n",
    "### before move \n",
    "-> J\n",
    "### after move\n",
    "-> E\n",
    "\n",
    "1. The name of the variable that contains 'fan' is\n",
    "### before remove \n",
    "-> J\n",
    "### after remove\n",
    "-> none\n",
    "\n",
    "\n",
    "1. The name of the variable that contains 'car' is\n",
    "### before put \n",
    "-> none\n",
    "### after put\n",
    "-> G\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-finetuning]",
   "language": "python",
   "name": "conda-env-.conda-finetuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
