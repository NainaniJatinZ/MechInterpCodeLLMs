{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4302d05d-77b2-43f1-a32b-74c88e05a8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jnainani_umass_edu/codellm/MechInterpCodeLLMs\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jnainani_umass_edu/codellm/MechInterpCodeLLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1072dfb-680d-4caa-85b6-cf3af9d5aa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "Primary device: cuda:0\n",
      "GPU 0: 81050.62 MB  Allocated: 0.00 MB  Reserved: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "from typing import List, Union, Optional, Callable\n",
    "from typing_extensions import Literal\n",
    "from functools import partial\n",
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "\n",
    "import argparse\n",
    "\n",
    "# THIS IS A LOCAL (MODIFIED) VERSION OF TRANSFORMER_LENS - UNINSTALL PIP/CONDA VERSION BEFORE USE!\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "import transformer_lens.patching as patching\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "\n",
    "import utils_patch.patch_helper_functions as helper\n",
    "\n",
    "import os\n",
    "from config import HF_TOKEN, HF_PATH\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = HF_PATH\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = HF_PATH\n",
    "os.environ[\"HF_HOME\"] = HF_PATH\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "device_id = 0\n",
    "if num_gpus > 0:\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "    \n",
    "def check_gpu_memory(max_alloc=0.9):\n",
    "    if not torch.cuda.is_available():\n",
    "        return\n",
    "    global device_id, device\n",
    "    print(\"Primary device:\", device)\n",
    "    torch.cuda.empty_cache()\n",
    "    max_alloc = 1 if max_alloc > 1 else max_alloc\n",
    "    for gpu in range(num_gpus):\n",
    "        memory_reserved = torch.cuda.memory_reserved(device=gpu)\n",
    "        memory_allocated = torch.cuda.memory_allocated(device=gpu)\n",
    "        total_memory = torch.cuda.get_device_properties(gpu).total_memory \n",
    "        print(f\"GPU {gpu}: {total_memory / (1024**2):.2f} MB  Allocated: {memory_allocated / (1024**2):.2f} MB  Reserved: {memory_reserved / (1024**2):.2f} MB\")\n",
    "                \n",
    "        # Check if the current GPU is getting too full, and if so we switch the primary device to the next GPU\n",
    "        if memory_reserved > max_alloc * total_memory:\n",
    "            if device_id < num_gpus - 1:\n",
    "                device_id += 1\n",
    "                device = f\"cuda:{device_id}\"\n",
    "                print(f\"Switching primary device to {device}\")\n",
    "            else:\n",
    "                print(\"Cannot switch primary device, all GPUs are nearly full\")\n",
    "\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8199f45b-6557-480a-8c5d-eb18e3520e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040cdb4010fe4ee4805bee4a1c4cd544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model CodeLlama-7b-hf into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"CodeLlama-7b-hf\", n_devices=num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "08db9aed-8559-4c74-b1bb-9cbe79685915",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"\"\"X = \"-\"\\nY = \"+\"\\nZ = \"*\"\\nW = \"\\\"\\nthe name of the variable with value \"*\" is \"\"\", \n",
    "           \"\"\"X = \"-\"\\nY = \"+\"\\nZ = \"*\"\\nW = \"\\\"\\nthe name of the variable with value \"+\" is \"\"\", \n",
    "           \"\"\"X = \"-\"\\nY = \"+\"\\nZ = \"*\"\\nW = \"\\\"\\nthe name of the variable with value \"-\" is \"\"\"]\n",
    "answers = [\"cat\", \"bird\", \"dog\", \"horse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4d2bdcc0-b4b6-41ae-b50e-6036f9b74f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df23e2d37f14d41a0b99b69b02226f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'lyh = \"-\"\\njnh = \"+\"\\nthj = \"*\"\\nbgd = \"\"\\nthe name of the variable with value \"*\" is 4\\n\\\\end{'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(prompts[0], max_new_tokens=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f244ab20-2b77-4f03-9cee-99c5930233a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([796], device='cuda:0')\n",
      "Z\n",
      "tensor([612], device='cuda:0')\n",
      "Y\n",
      "tensor([29896], device='cuda:0')\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts: \n",
    "    clean_tokens = model.to_tokens(prompt)\n",
    "    logits = model(clean_tokens)[:, -1, :]\n",
    "    model_answers = torch.argmax(logits, dim=-1)\n",
    "    print(model_answers)\n",
    "    print(model.to_string(model_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32c5417b-93df-4d0e-9601-612b0840db4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_next_token_accuracy(prompts, answers, model):\n",
    "    # total_examples = len(prompts)\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        clean_tokens = model.to_tokens(prompts)\n",
    "        logits = model(clean_tokens)[:, -1, :]\n",
    "        model_answers = torch.argmax(logits, dim=-1)\n",
    "    for ind in range(len(model_answers)):\n",
    "        if model.to_string(model_answers[ind]) == answers[ind]:\n",
    "            correct_predictions+=1\n",
    "    return correct_predictions / len(model_answers)\n",
    "\n",
    "evaluate_next_token_accuracy(prompts, answers, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9301c6e-335e-4f67-a9ea-0afe617136d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 2 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(prompts)[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m----> 2\u001b[0m predicted_token_id \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto_string(predicted_token_id)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 2 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "logits = model(prompts)[:, -1, :]\n",
    "predicted_token_id = torch.argmax(logits, dim=-1).item()\n",
    "model.to_string(predicted_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c7111a4-9639-4837-b42b-eff370e4778c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid shape passed in: torch.Size([1, 32, 32016])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codellm/MechInterpCodeLLMs/transformer_lens/HookedTransformer.py:780\u001b[0m, in \u001b[0;36mHookedTransformer.to_string\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(tokens, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape passed in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid shape passed in: torch.Size([1, 32, 32016])"
     ]
    }
   ],
   "source": [
    "model.to_string(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "26eed411-c41f-45a6-9c9e-884021ba5356",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/prompt_dataset_zero_shot.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Take the first few dictionaries (e.g., first 3)\n",
    "subset = data[9:]\n",
    "\n",
    "# Initialize lists\n",
    "prompts = []\n",
    "answers = []\n",
    "\n",
    "# Extract prompts and outputs\n",
    "outputs = []\n",
    "for ind, item in enumerate(subset):\n",
    "    # if ind in [0, 1, 3, 4, 5, 6, 7, 9]:\n",
    "    prompts.append(item[\"prompt\"])\n",
    "    outputs.append(item[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9ff653aa-b94c-4518-93dc-b638ca3d8f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \"-\"\n",
      "Y = \"+\"\n",
      "Z = \"*\"\n",
      "W = \"<\"\n",
      "the name of the variable with value \"*\" is \n"
     ]
    }
   ],
   "source": [
    "prompts = [\"\"\"X = \"-\"\\nY = \"+\"\\nZ = \"*\"\\nW = \"<\"\\nthe name of the variable with value \"*\" is \"\"\", \n",
    "           \"\"\"X = \"-\"\\nY = \"+\"\\nZ = \"*\"\\nW = \"<\"\\nthe name of the variable with value \"+\" is \"\"\", \n",
    "           \"\"\"X = \"-\"\\nY = \"+\"\\nZ = \"*\"\\nW = \"<\"\\nthe name of the variable with value \"-\" is \"\"\"]\n",
    "answers = [\"Z\", \"Y\", \"X\"]\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bb2b6898-257c-418c-8d3d-3e8c32b0ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_next_token_accuracy(prompts, answers, model):\n",
    "    # total_examples = len(prompts)\n",
    "    correct_predictions = 0\n",
    "    for ind in range(len(prompts)):\n",
    "        # print(prompts[ind])\n",
    "        with torch.no_grad():\n",
    "            clean_tokens = model.to_tokens(prompts[ind])\n",
    "            logits = model(clean_tokens)[:, -1, :]\n",
    "            model_answers = torch.argmax(logits, dim=-1)\n",
    "            print(\"pred: \",model.to_string(model_answers))\n",
    "            if model.to_string(model_answers) == answers[ind]:\n",
    "                # print(\"pred: \",model.to_string(model_answers))\n",
    "                print(\"ans: \",answers[ind])\n",
    "                correct_predictions+=1\n",
    "    return correct_predictions / len(prompts)\n",
    "\n",
    "# evaluate_next_token_accuracy(prompts, answers, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c63ac1d6-16f2-4cbd-98bf-a7716b61b0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:  Z\n",
      "ans:  Z\n",
      "pred:  Y\n",
      "ans:  Y\n",
      "pred:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_next_token_accuracy(prompts, answers, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fbede-ae63-4f66-b0a5-de8a6c0c75f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d2f8cd-8508-4eaa-a1b4-e1b337e88a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5ea67-b3ac-4b5a-ac35-79170b9d8ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c50402c-a9bf-4ac3-9ff9-7b90e369f202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d1d35b-e1ca-4679-8280-8d418d75538d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43a00f-e9f4-4402-85e5-9adb6ccfd939",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import json\n",
    "\n",
    "def generate_prompts(num_prompts=400):\n",
    "    operators = ['+', '-', '*', '**', '%', '<', '>', '<=', '>=', '==']\n",
    "    functions = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K']\n",
    "    prompts = []\n",
    "    \n",
    "    for _ in range(num_prompts):\n",
    "        selected_operators = random.sample(operators, 11)\n",
    "        selected_functions = random.sample(functions, 11)\n",
    "        \n",
    "        prompt = \"For this set of functions, return the function which uses the specified operator.\\n\\n\"\n",
    "        \n",
    "        examples = []\n",
    "        op_to_func_mapping = {}\n",
    "\n",
    "        for op, func in zip(selected_operators, selected_functions):\n",
    "            prompt += f\"def {func}(a, b):\\n\"\n",
    "            prompt += f\"    return a {op} b\\n\"\n",
    "            examples.append([func,op])\n",
    "            op_to_func_mapping[op] = func\n",
    "        \n",
    "\n",
    "        \n",
    "        prompt += f\"\\nExample: the name of the function that uses the '{examples[0][1]}' operator is \"\n",
    "        prompt += examples[0][0]\n",
    "        prompt += f\"\\nExample: the name of the function that uses the '{examples[1][1]}' operator is \"\n",
    "        prompt += examples[1][0]\n",
    "\n",
    "        selected_operators.remove(examples[0][1])\n",
    "        selected_operators.remove(examples[1][1])\n",
    "\n",
    "        chosen_op = random.sample(selected_operators, 1)[0]\n",
    "\n",
    "        prompt += f\"\\nExample: the name of the function that uses the '{chosen_op}' operator is \"\n",
    "\n",
    "        \n",
    "        prompts.append({\"prompt\": prompt, \"output\": op_to_func_mapping[chosen_op]})\n",
    "    \n",
    "    print(prompts[0]['prompt'])\n",
    "    print(prompts[0]['output'])\n",
    "    with open(\"prompts_dataset.json\", \"w\") as f:\n",
    "        json.dump(prompts, f, indent=4)\n",
    "\n",
    "generate_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fbd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import json\n",
    "\n",
    "def generate_prompts(num_prompts=400):\n",
    "    operators = ['+', '-', '*', '**', '%', '<', '>', '<=', '>=', '==']\n",
    "    functions = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K']\n",
    "    prompts = []\n",
    "    \n",
    "    for _ in range(num_prompts):\n",
    "        selected_operators = random.sample(operators, 11)\n",
    "        selected_functions = random.sample(functions, 11)\n",
    "        \n",
    "        prompt = \"For this set of functions, return the variable name containing the specified operator.\\n\\n\"\n",
    "        \n",
    "        examples = []\n",
    "        op_to_func_mapping = {}\n",
    "\n",
    "        for op, func in zip(selected_operators, selected_functions):\n",
    "            prompt += f\"{func} = {op}\\n\"\n",
    "            # prompt += f\"    return a {op} b\\n\"\n",
    "            examples.append([func,op])\n",
    "            op_to_func_mapping[op] = func\n",
    "        \n",
    "\n",
    "        \n",
    "        # prompt += f\"\\nExample: the name of the function that uses the '{examples[0][1]}' operator is \"\n",
    "        # prompt += examples[0][0]\n",
    "        # prompt += f\"\\nExample: the name of the function that uses the '{examples[1][1]}' operator is \"\n",
    "        # prompt += examples[1][0]\n",
    "\n",
    "        # selected_operators.remove(examples[0][1])\n",
    "        # selected_operators.remove(examples[1][1])\n",
    "\n",
    "        chosen_op = random.sample(selected_operators, 1)[0]\n",
    "\n",
    "        prompt += f\"\\nThe name of the variable that contains the '{chosen_op}' operator is \"\n",
    "\n",
    "        \n",
    "        prompts.append({\"prompt\": prompt, \"output\": op_to_func_mapping[chosen_op]})\n",
    "    \n",
    "    print(prompts[0]['prompt'])\n",
    "    print(prompts[0]['output'])\n",
    "    with open(\"prompts_dataset.json\", \"w\") as f:\n",
    "        json.dump(prompts, f, indent=4)\n",
    "\n",
    "generate_prompts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-finetuning]",
   "language": "python",
   "name": "conda-env-.conda-finetuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
