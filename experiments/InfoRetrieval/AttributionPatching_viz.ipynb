{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3b5be92-17aa-4e07-bfb1-3c7daf0dd980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you are at the root directory of repo\n",
    "project_root_name = \"MechInterpCodeLLMs\"\n",
    "\n",
    "# Find the project root directory\n",
    "def find_project_root(current_path, project_name):\n",
    "    while current_path != os.path.dirname(current_path):\n",
    "        if project_name in os.listdir(current_path):\n",
    "            return os.path.join(current_path, project_name)\n",
    "        current_path = os.path.dirname(current_path)\n",
    "    raise FileNotFoundError(f\"Project root directory '{project_name}' not found.\")\n",
    "\n",
    "# Get the path of the current notebook\n",
    "current_path = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "project_root = find_project_root(current_path, project_root_name)\n",
    "\n",
    "# Set the project root as the current working directory\n",
    "os.chdir(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63f8dc87-5de3-4c15-9842-f81f3b23ae75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md  config.py  \u001b[0m\u001b[01;34mdata\u001b[0m/  \u001b[01;34mexperiments\u001b[0m/  \u001b[01;34mlogs\u001b[0m/  \u001b[01;34mplots\u001b[0m/  \u001b[01;34mtransformer_lens\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "# Feel free to change above code, your current directory should look like this\n",
    "# README.md  config.py  data/  experiments/  logs/  plots/  transformer_lens/\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb887271-308b-4166-9349-9b6778b71590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a local version of transformer_lens, pip uninstall transformer_lens before reach here\n",
    "import transformer_lens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb5dcb5a-734f-40f9-95c6-713f20f6b000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3085483/3715311821.py:33: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "/tmp/ipykernel_3085483/3715311821.py:34: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "from typing import List, Union, Optional, Callable\n",
    "from typing_extensions import Literal\n",
    "from functools import partial\n",
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML, Markdown\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "ipython = get_ipython()\n",
    "ipython.magic(\"load_ext autoreload\")\n",
    "ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abaa3e57-7cfb-4249-aaa7-e3d79123143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS A LOCAL (MODIFIED) VERSION OF TRANSFORMER_LENS - UNINSTALL PIP/CONDA VERSION BEFORE USE!\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "import transformer_lens.patching as patching\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6615a520-aa6c-4751-bb8e-61c5fe537ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace bookkeeping so that the home folder doesn't get filled up\n",
    "# I have these in my .bashrc and .zshrc and they work in terminal but not recognized by python for some reason\n",
    "\n",
    "# You dont these to run gpt2-small, if you dont want to mess with hf, just comment this\n",
    "\n",
    "import os\n",
    "from config import HF_TOKEN, HF_PATH\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = HF_PATH\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = HF_PATH\n",
    "os.environ[\"HF_HOME\"] = HF_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59f5d5aa-7be1-4c24-a0f5-c75f2161f2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 0.00 MB  Reserved: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# When using multiple GPUs we use GPU 0 as the primary and switch to the next when it is 90% full\n",
    "num_gpus = torch.cuda.device_count()\n",
    "device_id = 0\n",
    "if num_gpus > 0:\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "def check_gpu_memory(max_alloc=0.9):\n",
    "    if not torch.cuda.is_available():\n",
    "        return\n",
    "    global device_id, device\n",
    "    print(\"Primary device:\", device)\n",
    "    torch.cuda.empty_cache()\n",
    "    max_alloc = 1 if max_alloc > 1 else max_alloc\n",
    "    for gpu in range(num_gpus):\n",
    "        memory_reserved = torch.cuda.memory_reserved(device=gpu)\n",
    "        memory_allocated = torch.cuda.memory_allocated(device=gpu)\n",
    "        total_memory = torch.cuda.get_device_properties(gpu).total_memory \n",
    "        print(f\"GPU {gpu}: {total_memory / (1024**2):.2f} MB  Allocated: {memory_allocated / (1024**2):.2f} MB  Reserved: {memory_reserved / (1024**2):.2f} MB\")\n",
    "                \n",
    "        # Check if the current GPU is getting too full, and if so we switch the primary device to the next GPU\n",
    "        if memory_reserved > max_alloc * total_memory:\n",
    "            if device_id < num_gpus - 1:\n",
    "                device_id += 1\n",
    "                device = f\"cuda:{device_id}\"\n",
    "                print(f\"Switching primary device to {device}\")\n",
    "            else:\n",
    "                print(\"Cannot switch primary device, all GPUs are nearly full\")\n",
    "\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "check_gpu_memory()\n",
    "\n",
    "def timeit(func):\n",
    "    \"\"\"Decorator to measure the execution time of a function.\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Function {func.__name__!r} executed in {end_time - start_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b756d9ab-2d79-46ba-9f07-7e80355dedeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d85ab1d23e841dca0d6678cdf22d0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc56f8ba8824994b17ef48328941df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a220487d71ef442cb28c7bf852b19c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc495fed81f14f8184d97448877a8692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96dc8160f8f44b5eaf86ce4fe4340029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58763a925874866915f3a7dc61af9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f243a1cf484be297f769b1a13ed15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 635.51 MB  Reserved: 690.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Create transformer\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", n_devices=num_gpus)\n",
    "\n",
    "# We need these so that individual attention heads and MLP inputs can be edited\n",
    "model.set_use_attn_in(True)\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_attn_result(True) # Documentation says this easily burns through GPU memory\n",
    "\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d894051d-9a86-4d8e-9fc0-23d2d2c02bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean string 0 <|endoftext|>When J and M went to the shops, J gave the bag to \n",
      "Corrupted string 0 <|endoftext|>When J and M went to the shops, M gave the bag to \n",
      "Answer token indices tensor([[44, 41],\n",
      "        [41, 44],\n",
      "        [51, 41],\n",
      "        [41, 51],\n",
      "        [35, 50],\n",
      "        [50, 35],\n",
      "        [49, 32],\n",
      "        [32, 49]], device='cuda:0')\n",
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 635.52 MB  Reserved: 690.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Some examples to work with - replace with code examples\n",
    "\n",
    "prompts = ['When J and M went to the shops, J gave the bag to ', 'When J and M went to the shops, M gave the bag to ', 'When T and J went to the park, J gave the ball to ', 'When T and J went to the park, T gave the ball to ', 'When D and S went to the shops, S gave an apple to ', 'When D and S went to the shops, D gave an apple to ', 'After R and A went to the park, A gave a drink to ', 'After R and A went to the park, R gave a drink to ']\n",
    "answers = [('M', 'J'), ('J', 'M'), ('T', 'J'), ('J', 'T'), ('D', 'S'), ('S', 'D'), ('R', 'A'), ('A', 'R')]\n",
    "\n",
    "clean_tokens = model.to_tokens(prompts)\n",
    "# Swap each adjacent pair, with a hacky list comprehension\n",
    "corrupted_tokens = clean_tokens[\n",
    "    [(i+1 if i%2==0 else i-1) for i in range(len(clean_tokens)) ]\n",
    "    ]\n",
    "print(\"Clean string 0\", model.to_string(clean_tokens[0]))\n",
    "print(\"Corrupted string 0\", model.to_string(corrupted_tokens[0]))\n",
    "\n",
    "answer_token_indices = torch.tensor([[model.to_single_token(answers[i][j]) for j in range(2)] for i in range(len(answers))], device=device)\n",
    "print(\"Answer token indices\", answer_token_indices)\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4453f87-017c-468b-ac5c-97641992c7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean logit diff: 0.1306\n",
      "Corrupted logit diff: -0.1306\n",
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1916.72 MB  Reserved: 1936.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Logit difference metric\n",
    "def get_logit_diff(logits, answer_token_indices=answer_token_indices, device=\"cpu\"):\n",
    "    if len(logits.shape) == 3:\n",
    "        # Get final logits only\n",
    "        logits = logits[:, -1, :]\n",
    "    logits = logits.to(answer_token_indices.device)\n",
    "    correct_logits = logits.gather(1, answer_token_indices[:, 0].unsqueeze(1))\n",
    "    incorrect_logits = logits.gather(1, answer_token_indices[:, 1].unsqueeze(1))\n",
    "    return (correct_logits - incorrect_logits).mean()\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "\n",
    "clean_logit_diff = get_logit_diff(clean_logits, answer_token_indices).item()\n",
    "print(f\"Clean logit diff: {clean_logit_diff:.4f}\")\n",
    "\n",
    "corrupted_logit_diff = get_logit_diff(corrupted_logits, answer_token_indices).item()\n",
    "print(f\"Corrupted logit diff: {corrupted_logit_diff:.4f}\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "517b36a9-7ff8-42d1-9380-d67fce09e6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Baseline is 1: 1.0000\n",
      "Corrupted Baseline is 0: 0.0000\n",
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1916.72 MB  Reserved: 1936.00 MB\n"
     ]
    }
   ],
   "source": [
    "# IOI metric - does this mean indirect object identification? Yes\n",
    "\n",
    "CLEAN_BASELINE = clean_logit_diff\n",
    "CORRUPTED_BASELINE = corrupted_logit_diff\n",
    "\n",
    "def ioi_metric(logits, answer_token_indices=answer_token_indices):\n",
    "    logits = logits.to(device)\n",
    "    torch.cuda.empty_cache()\n",
    "    return (get_logit_diff(logits, answer_token_indices) - CORRUPTED_BASELINE) / (\n",
    "        CLEAN_BASELINE - CORRUPTED_BASELINE\n",
    "    )\n",
    "\n",
    "print(f\"Clean Baseline is 1: {ioi_metric(clean_logits).item():.4f}\")\n",
    "print(f\"Corrupted Baseline is 0: {ioi_metric(corrupted_logits).item():.4f}\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29f3130d-3ec4-423a-abc2-69beec2b967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metric = Callable[[TT[\"batch_and_pos_dims\", \"d_model\"]], float]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52c3bf5-fc32-4a5b-94ed-ef3984f1f11e",
   "metadata": {},
   "source": [
    "## Removing prev cache\n",
    "The memory shot up during run_with_cache, which I dont think we need later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "229423cc-b22d-4b2a-93f0-782bacfa3bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 643.64 MB  Reserved: 710.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Delete tensors\n",
    "del clean_logits\n",
    "del corrupted_logits\n",
    "del clean_logit_diff \n",
    "del corrupted_logit_diff\n",
    "clean_cache = clean_cache.to(\"cpu\")\n",
    "del corrupted_cache\n",
    "\n",
    "# Empty CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Optionally check memory to confirm\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a4b13-698e-4802-a378-cf3ab900f551",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Attribution patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2256c670-9c9c-4fcc-95e4-4e7ddd682804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Value: 1.0\n",
      "Clean Activations Cached: 244\n",
      "Clean Gradients Cached: 244\n",
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1720.93 MB  Reserved: 1836.00 MB\n",
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1276.02 MB  Reserved: 1740.00 MB\n",
      "Corrupted Value: 0.0\n",
      "Corrupted Activations Cached: 244\n",
      "Corrupted Gradients Cached: 244\n",
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1727.43 MB  Reserved: 1956.00 MB\n",
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1276.02 MB  Reserved: 1740.00 MB\n"
     ]
    }
   ],
   "source": [
    "filter_not_qkv_input = lambda name: \"_input\" not in name\n",
    "\n",
    "def get_cache_fwd_and_bwd(model, tokens, metric):\n",
    "    model.reset_hooks()\n",
    "    cache = {}\n",
    "\n",
    "    def forward_cache_hook(act, hook):\n",
    "        act = act.to(device)\n",
    "        torch.cuda.empty_cache()\n",
    "        cache[hook.name] = act\n",
    "\n",
    "    model.add_hook(filter_not_qkv_input, forward_cache_hook, \"fwd\")\n",
    "    grad_cache = {}\n",
    "\n",
    "    def backward_cache_hook(act, hook):\n",
    "        act = act.to(device)\n",
    "        torch.cuda.empty_cache()\n",
    "        grad_cache[hook.name] = act\n",
    "\n",
    "    model.add_hook(filter_not_qkv_input, backward_cache_hook, \"bwd\")\n",
    "    \n",
    "    result = model(tokens).to(device)\n",
    "    torch.cuda.empty_cache()\n",
    "    value = metric(result)\n",
    "    value.backward()\n",
    "\n",
    "    # Reset hooks and clear unused GPU memory\n",
    "    value = value.item()\n",
    "    model.reset_hooks()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    cache = ActivationCache(cache, model).to(device)\n",
    "    grad_cache = ActivationCache(grad_cache, model).to(device)\n",
    "    \n",
    "    return value,cache, grad_cache\n",
    "\n",
    "clean_value, clean_cache, clean_grad_cache = get_cache_fwd_and_bwd(model, clean_tokens, ioi_metric)\n",
    "print(\"Clean Value:\", clean_value)\n",
    "print(\"Clean Activations Cached:\", len(clean_cache))\n",
    "print(\"Clean Gradients Cached:\", len(clean_grad_cache))\n",
    "check_gpu_memory()\n",
    "\n",
    "clean_cache = clean_cache.to('cpu')\n",
    "clean_grad_cache = clean_grad_cache.to('cpu')\n",
    "\n",
    "check_gpu_memory()\n",
    "\n",
    "corrupted_value, corrupted_cache, corrupted_grad_cache = get_cache_fwd_and_bwd(model, corrupted_tokens, ioi_metric)\n",
    "print(\"Corrupted Value:\", corrupted_value)\n",
    "print(\"Corrupted Activations Cached:\", len(corrupted_cache))\n",
    "print(\"Corrupted Gradients Cached:\", len(corrupted_grad_cache))\n",
    "check_gpu_memory()\n",
    "\n",
    "corrupted_cache = corrupted_cache.to('cpu')\n",
    "corrupted_grad_cache = corrupted_grad_cache.to('cpu')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "150cb74d-2ee3-4e9b-887d-7b0deb545a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L0H0', 'L0H1', 'L0H2', 'L0H3', 'L0H4']\n",
      "['L0H0+', 'L0H0-', 'L0H1+', 'L0H1-', 'L0H2+']\n",
      "['L0H0Q', 'L0H0K', 'L0H0V', 'L0H1Q', 'L0H1K']\n"
     ]
    }
   ],
   "source": [
    "HEAD_NAMES = [\n",
    "    f\"L{l}H{h}\" for l in range(model.cfg.n_layers) for h in range(model.cfg.n_heads)\n",
    "]\n",
    "HEAD_NAMES_SIGNED = [f\"{name}{sign}\" for name in HEAD_NAMES for sign in [\"+\", \"-\"]]\n",
    "HEAD_NAMES_QKV = [\n",
    "    f\"{name}{act_name}\" for name in HEAD_NAMES for act_name in [\"Q\", \"K\", \"V\"]\n",
    "]\n",
    "print(HEAD_NAMES[:5])\n",
    "print(HEAD_NAMES_SIGNED[:5])\n",
    "print(HEAD_NAMES_QKV[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86e5a643-dc88-4ab1-b94a-153d5fc46803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'create_attention_attr' executed in 0.2138 seconds\n",
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1276.02 MB  Reserved: 1740.00 MB\n"
     ]
    }
   ],
   "source": [
    "@timeit\n",
    "def create_attention_attr(\n",
    "    clean_cache, clean_grad_cache, device\n",
    ") -> TT[\"batch\", \"layer\", \"head_index\", \"dest\", \"src\"]:\n",
    "    attention_stack = torch.stack(\n",
    "        [clean_cache[\"pattern\", l] for l in range(model.cfg.n_layers)], dim=0\n",
    "    ).to(device)\n",
    "    attention_grad_stack = torch.stack(\n",
    "        [clean_grad_cache[\"pattern\", l] for l in range(model.cfg.n_layers)], dim=0\n",
    "    ).to(device)\n",
    "    attention_attr = attention_grad_stack * attention_stack\n",
    "    attention_attr = einops.rearrange(\n",
    "        attention_attr,\n",
    "        \"layer batch head_index dest src -> batch layer head_index dest src\",\n",
    "    )\n",
    "    return attention_attr\n",
    "\n",
    "attention_attr = create_attention_attr(clean_cache, clean_grad_cache, \"cpu\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c765b5e-e5d2-4fbb-a08b-bf3603132917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1276.02 MB  Reserved: 1740.00 MB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6070204-26e4-4cfe-b667-143630f7bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : viz corresponding to plot_attention_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a2fe9f-f172-4d41-952b-b833b3decb47",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Residual Stream Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86cbd6e1-25fd-458a-88a2-98b741f77242",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def attr_patch_residual(\n",
    "    clean_cache: ActivationCache,\n",
    "    corrupted_cache: ActivationCache,\n",
    "    corrupted_grad_cache: ActivationCache,\n",
    "    device,\n",
    ") -> TT[\"component\", \"pos\"]:\n",
    "    clean_residual, residual_labels = clean_cache.accumulated_resid(\n",
    "        -1, incl_mid=True, return_labels=True\n",
    "    )\n",
    "    corrupted_residual = corrupted_cache.accumulated_resid(\n",
    "        -1, incl_mid=True, return_labels=False\n",
    "    )\n",
    "    corrupted_grad_residual = corrupted_grad_cache.accumulated_resid(\n",
    "        -1, incl_mid=True, return_labels=False\n",
    "    )\n",
    "    residual_attr = einops.reduce(\n",
    "        corrupted_grad_residual * (clean_residual - corrupted_residual),\n",
    "        \"component batch pos d_model -> component pos\",\n",
    "        \"sum\",\n",
    "    ).to(device)\n",
    "    return residual_attr, residual_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7b78686-f90d-47ba-8102-0dc0e1502da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'attr_patch_residual' executed in 0.0241 seconds\n",
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1276.02 MB  Reserved: 1740.00 MB\n"
     ]
    }
   ],
   "source": [
    "residual_attr, residual_labels = attr_patch_residual(\n",
    "    clean_cache, corrupted_cache, corrupted_grad_cache, \"cpu\"\n",
    ")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de397e-bae6-463f-beec-08f436b312c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : viz corresponding to attr_patch_residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb85eecd-1f94-43ba-8eec-c882c3e29e0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Layer Output Attribution Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b0e9a8e-2ab6-44ea-b6e9-2fe397b2281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def attr_patch_layer_out(\n",
    "    clean_cache: ActivationCache,\n",
    "    corrupted_cache: ActivationCache,\n",
    "    corrupted_grad_cache: ActivationCache,\n",
    "    device\n",
    ") -> TT[\"component\", \"pos\"]:\n",
    "    clean_layer_out, labels = clean_cache.decompose_resid(-1, return_labels=True)\n",
    "    corrupted_layer_out = corrupted_cache.decompose_resid(-1, return_labels=False)\n",
    "    corrupted_grad_layer_out = corrupted_grad_cache.decompose_resid(\n",
    "        -1, return_labels=False\n",
    "    )\n",
    "    layer_out_attr = einops.reduce(\n",
    "        corrupted_grad_layer_out * (clean_layer_out - corrupted_layer_out),\n",
    "        \"component batch pos d_model -> component pos\",\n",
    "        \"sum\",\n",
    "    ).to(device)\n",
    "    return layer_out_attr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa052b74-1ffa-4f17-ad62-1d4a38c77c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'attr_patch_layer_out' executed in 0.0250 seconds\n",
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1276.02 MB  Reserved: 1740.00 MB\n"
     ]
    }
   ],
   "source": [
    "layer_out_attr, layer_out_labels = attr_patch_layer_out(\n",
    "    clean_cache, corrupted_cache, corrupted_grad_cache, \"cpu\"\n",
    ")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c304c0-a614-46ff-ade7-2356db02ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : viz corresponding to attr_patch_layer_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb51e6-18d2-4865-95f8-4c5f987d4494",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Head output attribution Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fbb2dbc3-9991-4169-8b0e-b54ef738a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def attr_patch_head_out(\n",
    "    clean_cache: ActivationCache,\n",
    "    corrupted_cache: ActivationCache,\n",
    "    corrupted_grad_cache: ActivationCache,\n",
    "    device\n",
    ") -> TT[\"component\", \"pos\"]:\n",
    "    labels = HEAD_NAMES\n",
    "\n",
    "    clean_head_out = clean_cache.stack_head_results(-1, return_labels=False).to(device)\n",
    "    corrupted_head_out = corrupted_cache.stack_head_results(-1, return_labels=False).to(device)\n",
    "    corrupted_grad_head_out = corrupted_grad_cache.stack_head_results(\n",
    "        -1, return_labels=False\n",
    "    ).to(device)\n",
    "    head_out_attr = einops.reduce(\n",
    "        corrupted_grad_head_out * (clean_head_out - corrupted_head_out),\n",
    "        \"component batch pos d_model -> component pos\",\n",
    "        \"sum\",\n",
    "    ).to(device)\n",
    "    return head_out_attr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b31566a2-c5ce-4698-9418-ca25b894f558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'attr_patch_head_out' executed in 0.1252 seconds\n",
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1276.02 MB  Reserved: 1740.00 MB\n"
     ]
    }
   ],
   "source": [
    "head_out_attr, head_out_labels = attr_patch_head_out(\n",
    "    clean_cache, corrupted_cache, corrupted_grad_cache, \"cpu\"\n",
    ")\n",
    "\n",
    "sum_head_out_attr = einops.reduce(\n",
    "    head_out_attr,\n",
    "    \"(layer head) pos -> layer head\",\n",
    "    \"sum\",\n",
    "    layer=model.cfg.n_layers,\n",
    "    head=model.cfg.n_heads,\n",
    ")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c958159-1903-4fe5-9dea-f8eff7cab28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : viz corresponding to attr_patch_head_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39523830-9748-4aed-bfd1-daeae5a99831",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Head activation patching, skipping it for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5a80684-b1a2-44bd-931e-db98d2d5819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_head_vector_from_cache(\n",
    "    cache, activation_name: Literal[\"q\", \"k\", \"v\", \"z\"], device\n",
    ") -> TT[\"layer_and_head_index\", \"batch\", \"pos\", \"d_head\"]:\n",
    "    \"\"\"Stacks the head vectors from the cache from a specific activation (key, query, value or mixed_value (z)) into a single tensor.\"\"\"\n",
    "    stacked_head_vectors = torch.stack(\n",
    "        [cache[activation_name, l] for l in range(model.cfg.n_layers)], dim=0\n",
    "    ).to(device)\n",
    "    stacked_head_vectors = einops.rearrange(\n",
    "        stacked_head_vectors,\n",
    "        \"layer batch pos head_index d_head -> (layer head_index) batch pos d_head\",\n",
    "    ).to(device)\n",
    "    return stacked_head_vectors\n",
    "\n",
    "@timeit\n",
    "def attr_patch_head_vector(\n",
    "    clean_cache: ActivationCache,\n",
    "    corrupted_cache: ActivationCache,\n",
    "    corrupted_grad_cache: ActivationCache,\n",
    "    activation_name: Literal[\"q\", \"k\", \"v\", \"z\"],\n",
    "    device\n",
    ") -> TT[\"component\", \"pos\"]:\n",
    "    labels = HEAD_NAMES\n",
    "\n",
    "    clean_head_vector = stack_head_vector_from_cache(clean_cache, activation_name, \"cpu\").to(device)\n",
    "    corrupted_head_vector = stack_head_vector_from_cache(\n",
    "        corrupted_cache, activation_name, \"cpu\"\n",
    "    ).to(device)\n",
    "    corrupted_grad_head_vector = stack_head_vector_from_cache(\n",
    "        corrupted_grad_cache, activation_name, \"cpu\"\n",
    "    ).to(device)\n",
    "    head_vector_attr = einops.reduce(\n",
    "        corrupted_grad_head_vector * (clean_head_vector - corrupted_head_vector),\n",
    "        \"component batch pos d_head -> component pos\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    return head_vector_attr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d3fdbb1-7b60-4ba2-a263-062926db7689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'attr_patch_head_vector' executed in 0.0160 seconds\n",
      "Function 'attr_patch_head_vector' executed in 0.0051 seconds\n",
      "Function 'attr_patch_head_vector' executed in 0.0053 seconds\n",
      "Function 'attr_patch_head_vector' executed in 0.0088 seconds\n",
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1276.02 MB  Reserved: 1740.00 MB\n"
     ]
    }
   ],
   "source": [
    "head_vector_attr_dict = {}\n",
    "for activation_name, activation_name_full in [\n",
    "    (\"k\", \"Key\"),\n",
    "    (\"q\", \"Query\"),\n",
    "    (\"v\", \"Value\"),\n",
    "    (\"z\", \"Mixed Value\"),\n",
    "]:\n",
    "    head_vector_attr_dict[activation_name], head_vector_labels = attr_patch_head_vector(\n",
    "        clean_cache, corrupted_cache, corrupted_grad_cache, activation_name, \"cpu\"\n",
    "    )\n",
    "    sum_head_vector_attr = einops.reduce(\n",
    "        head_vector_attr_dict[activation_name],\n",
    "        \"(layer head) pos -> layer head\",\n",
    "        \"sum\",\n",
    "        layer=model.cfg.n_layers,\n",
    "        head=model.cfg.n_heads,\n",
    "    )\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a9eec-0fe2-4312-b68b-b9c9bd752dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : viz corresponding to attr_patch_head_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1f048-e679-4c22-ae27-1e04cfa8461d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Head Pattern Attribution Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "acabfed1-140d-4619-88a4-a1e42e06f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_head_pattern_from_cache(\n",
    "    cache,\n",
    "    device\n",
    ") -> TT[\"layer_and_head_index\", \"batch\", \"dest_pos\", \"src_pos\"]:\n",
    "    \"\"\"Stacks the head patterns from the cache into a single tensor.\"\"\"\n",
    "    stacked_head_pattern = torch.stack(\n",
    "        [cache[\"pattern\", l] for l in range(model.cfg.n_layers)], dim=0\n",
    "    ).to(device)\n",
    "    stacked_head_pattern = einops.rearrange(\n",
    "        stacked_head_pattern,\n",
    "        \"layer batch head_index dest_pos src_pos -> (layer head_index) batch dest_pos src_pos\",\n",
    "    ).to(device)\n",
    "    return stacked_head_pattern\n",
    "\n",
    "@timeit\n",
    "def attr_patch_head_pattern(\n",
    "    clean_cache: ActivationCache,\n",
    "    corrupted_cache: ActivationCache,\n",
    "    corrupted_grad_cache: ActivationCache,\n",
    "    device\n",
    ") -> TT[\"component\", \"dest_pos\", \"src_pos\"]:\n",
    "    labels = HEAD_NAMES\n",
    "\n",
    "    clean_head_pattern = stack_head_pattern_from_cache(clean_cache, \"cpu\").to(device)\n",
    "    corrupted_head_pattern = stack_head_pattern_from_cache(corrupted_cache, \"cpu\").to(device)\n",
    "    corrupted_grad_head_pattern = stack_head_pattern_from_cache(corrupted_grad_cache, \"cpu\").to(device)\n",
    "    head_pattern_attr = einops.reduce(\n",
    "        corrupted_grad_head_pattern * (clean_head_pattern - corrupted_head_pattern),\n",
    "        \"component batch dest_pos src_pos -> component dest_pos src_pos\",\n",
    "        \"sum\",\n",
    "    ).to(device)\n",
    "    return head_pattern_attr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2fc558f7-dc81-419f-bc45-b1aeb2da85e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'attr_patch_head_pattern' executed in 0.0028 seconds\n",
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1276.02 MB  Reserved: 1740.00 MB\n"
     ]
    }
   ],
   "source": [
    "head_pattern_attr, labels = attr_patch_head_pattern(\n",
    "    clean_cache, corrupted_cache, corrupted_grad_cache, \"cpu\"\n",
    ")\n",
    "\n",
    "head_pattern_attr = einops.rearrange(\n",
    "        head_pattern_attr,\n",
    "        \"(layer head) dest src -> layer head dest src\",\n",
    "        layer=model.cfg.n_layers,\n",
    "        head=model.cfg.n_heads,\n",
    "    )\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98c62c46-d05d-43e6-912f-af8b608e18a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : viz corresponding to attr_patch_head_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d05db8-d81f-463a-99ef-f5e5c2fc7320",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Head Path Attribution Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3737193-1d72-4242-b9d0-adbfa2ba8393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_vector_grad_input_from_grad_cache(\n",
    "    grad_cache: ActivationCache, activation_name: Literal[\"q\", \"k\", \"v\"], layer: int, device\n",
    ") -> TT[\"batch\", \"pos\", \"head_index\", \"d_model\"]:\n",
    "    vector_grad = grad_cache[activation_name, layer].to(device)\n",
    "    ln_scales = grad_cache[\"scale\", layer, \"ln1\"].to(device)\n",
    "    attn_layer_object = model.blocks[layer].attn\n",
    "    if activation_name == \"q\":\n",
    "        W = attn_layer_object.W_Q.to(device)\n",
    "    elif activation_name == \"k\":\n",
    "        W = attn_layer_object.W_K.to(device)\n",
    "    elif activation_name == \"v\":\n",
    "        W = attn_layer_object.W_V.to(device)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid activation name\")\n",
    "\n",
    "    # Original notebook used (batch pos) for second input but that seems to be wrong - double check this computation\n",
    "    return einsum(\n",
    "        \"batch pos head_index d_head, batch pos head_index, head_index d_model d_head -> batch pos head_index d_model\",\n",
    "        vector_grad,\n",
    "        ln_scales.squeeze(-1),\n",
    "        W,\n",
    "    )\n",
    "\n",
    "def get_stacked_head_vector_grad_input(\n",
    "    grad_cache, activation_name: Literal[\"q\", \"k\", \"v\"], device\n",
    ") -> TT[\"layer\", \"batch\", \"pos\", \"head_index\", \"d_model\"]:\n",
    "    return torch.stack(\n",
    "        [\n",
    "            get_head_vector_grad_input_from_grad_cache(grad_cache, activation_name, l, \"cpu\")\n",
    "            for l in range(model.cfg.n_layers)\n",
    "        ],\n",
    "        dim=0,\n",
    "    ).to(device)\n",
    "\n",
    "def get_full_vector_grad_input(\n",
    "    grad_cache, device\n",
    ") -> TT[\"qkv\", \"layer\", \"batch\", \"pos\", \"head_index\", \"d_model\"]:\n",
    "    return torch.stack([get_stacked_head_vector_grad_input(grad_cache, activation_name, \"cpu\").to(device) for activation_name in [\"q\", \"k\", \"v\"]], dim=0).to(device)\n",
    "\n",
    "@timeit\n",
    "def attr_patch_head_path(\n",
    "    clean_cache: ActivationCache,\n",
    "    corrupted_cache: ActivationCache,\n",
    "    corrupted_grad_cache: ActivationCache,\n",
    "    device\n",
    ") -> TT[\"qkv\", \"dest_component\", \"src_component\", \"pos\"]:\n",
    "    \"\"\"\n",
    "    Computes the attribution patch along the path between each pair of heads.\n",
    "\n",
    "    Sets this to zero for the path from any late head to any early head\n",
    "\n",
    "    \"\"\"\n",
    "    start_labels = HEAD_NAMES\n",
    "    end_labels = HEAD_NAMES_QKV\n",
    "    full_vector_grad_input = get_full_vector_grad_input(corrupted_grad_cache, \"cpu\")\n",
    "    clean_head_result_stack = clean_cache.stack_head_results(-1)\n",
    "    corrupted_head_result_stack = corrupted_cache.stack_head_results(-1)\n",
    "    diff_head_result = einops.rearrange(\n",
    "        clean_head_result_stack - corrupted_head_result_stack,\n",
    "        \"(layer head_index) batch pos d_model -> layer batch pos head_index d_model\",\n",
    "        layer=model.cfg.n_layers,\n",
    "        head_index=model.cfg.n_heads,\n",
    "    )\n",
    "    path_attr = einsum(\n",
    "        \"qkv layer_end batch pos head_end d_model, layer_start batch pos head_start d_model -> qkv layer_end head_end layer_start head_start pos\",\n",
    "        full_vector_grad_input,\n",
    "        diff_head_result,\n",
    "    )\n",
    "    correct_layer_order_mask = (\n",
    "        torch.arange(model.cfg.n_layers)[None, :, None, None, None, None]\n",
    "        > torch.arange(model.cfg.n_layers)[None, None, None, :, None, None]\n",
    "    ).to(path_attr.device)\n",
    "    zero = torch.zeros(1, device=path_attr.device)\n",
    "    path_attr = torch.where(correct_layer_order_mask, path_attr, zero)\n",
    "\n",
    "    path_attr = einops.rearrange(\n",
    "        path_attr,\n",
    "        \"qkv layer_end head_end layer_start head_start pos -> (layer_end head_end qkv) (layer_start head_start) pos\",\n",
    "    )\n",
    "    return path_attr, end_labels, start_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fee61b64-e942-4159-ad33-1dee88246c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'attr_patch_head_path' executed in 0.5152 seconds\n"
     ]
    }
   ],
   "source": [
    "head_path_attr, end_labels, start_labels = attr_patch_head_path(\n",
    "    clean_cache, corrupted_cache, corrupted_grad_cache, \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dca5f72c-931c-4fe6-94c0-33d963d87966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1276.02 MB  Reserved: 1740.00 MB\n"
     ]
    }
   ],
   "source": [
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1f6ccdcb-43d5-48f8-be6f-acc093955c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : viz corresponding to attr_patch_head_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ef4d5c1-7ee8-4e06-893b-05c17b4a2c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1276.02 MB  Reserved: 1740.00 MB\n"
     ]
    }
   ],
   "source": [
    "head_out_values, head_out_indices = head_out_attr.sum(-1).abs().sort(descending=True)\n",
    "top_head_indices = head_out_indices[:22].sort().values\n",
    "top_end_indices = []\n",
    "top_end_labels = []\n",
    "top_start_indices = []\n",
    "top_start_labels = []\n",
    "\n",
    "for i in top_head_indices:\n",
    "    i = i.item()\n",
    "    top_start_indices.append(i)\n",
    "    top_start_labels.append(start_labels[i])\n",
    "    for j in range(3):\n",
    "        top_end_indices.append(3 * i + j)\n",
    "        top_end_labels.append(end_labels[3 * i + j])\n",
    "        \n",
    "top_head_path_attr = einops.rearrange(\n",
    "    head_path_attr[top_end_indices, :][:, top_start_indices].sum(-1),\n",
    "    \"(head_end qkv) head_start -> qkv head_end head_start\",\n",
    "    qkv=3,\n",
    ")\n",
    "\n",
    "check_gpu_memory()\n",
    "\n",
    "# TODO : viz corresponding to attr_patch_head_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c2852b16-bf86-4157-9d72-774e247b0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : viz corresponding to attr_patch_head_path\n",
    "# for j, composition_type in enumerate([\"Query\", \"Key\", \"Value\"]):\n",
    "#     imshow(\n",
    "#         head_path_attr[top_end_indices, :][:, top_start_indices][j::3].sum(-1),\n",
    "#         y=top_end_labels[j::3],\n",
    "#         yaxis=\"Path End (Head Input)\",\n",
    "#         x=top_start_labels,\n",
    "#         xaxis=\"Path Start (Head Output)\",\n",
    "#         title=f\"Head Path to {composition_type} Attribution Patching (Filtered for Top Heads)\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0fd4da5e-fa02-4641-a53a-9919f1e42c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_head_path_attr = einops.rearrange(\n",
    "    head_path_attr[top_end_indices, :][:, top_start_indices].sum(-1),\n",
    "    \"(head_end qkv) head_start -> qkv head_end head_start\",\n",
    "    qkv=3,\n",
    ")\n",
    "# TODO : viz corresponding to attr_patch_head_path\n",
    "# imshow(\n",
    "#     top_head_path_attr,\n",
    "#     y=[i[:-1] for i in top_end_labels[::3]],\n",
    "#     yaxis=\"Path End (Head Input)\",\n",
    "#     x=top_start_labels,\n",
    "#     xaxis=\"Path Start (Head Output)\",\n",
    "#     title=f\"Head Path Attribution Patching (Filtered for Top Heads)\",\n",
    "#     facet_col=0,\n",
    "#     facet_labels=[\"Query\", \"Key\", \"Value\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "083e15df-c306-4f9f-a3a6-52c5b79b9a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1276.02 MB  Reserved: 1740.00 MB\n"
     ]
    }
   ],
   "source": [
    "interesting_heads = [\n",
    "    5 * model.cfg.n_heads + 5,\n",
    "    8 * model.cfg.n_heads + 6,\n",
    "    9 * model.cfg.n_heads + 9,\n",
    "]\n",
    "interesting_head_labels = [HEAD_NAMES[i] for i in interesting_heads]\n",
    "for head_index, label in zip(interesting_heads, interesting_head_labels):\n",
    "    in_paths = head_path_attr[3 * head_index : 3 * head_index + 3].sum(-1)\n",
    "    out_paths = head_path_attr[:, head_index].sum(-1)\n",
    "    out_paths = einops.rearrange(out_paths, \"(layer_head qkv) -> qkv layer_head\", qkv=3)\n",
    "    all_paths = torch.cat([in_paths, out_paths], dim=0)\n",
    "    all_paths = einops.rearrange(\n",
    "        all_paths,\n",
    "        \"path_type (layer head) -> path_type layer head\",\n",
    "        layer=model.cfg.n_layers,\n",
    "        head=model.cfg.n_heads,\n",
    "    )\n",
    "# TODO - implement visualization for input/output paths per head\n",
    "    \n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3f80b-11fd-4397-bf72-701d7423c7a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Validating Attribution vs Activation Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab4327d3-9363-4ea6-a638-99ad86a6e446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1276.02 MB  Reserved: 1740.00 MB\n"
     ]
    }
   ],
   "source": [
    "attribution_cache_dict = {}\n",
    "for key in corrupted_grad_cache.cache_dict.keys():\n",
    "    attribution_cache_dict[key] = corrupted_grad_cache.cache_dict[key] * (\n",
    "        clean_cache.cache_dict[key] - corrupted_cache.cache_dict[key]\n",
    "    ).to(\"cpu\")\n",
    "attr_cache = ActivationCache(attribution_cache_dict, model).to(\"cpu\")\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c1862-c4fb-4dac-8e57-7bd52af0bc3f",
   "metadata": {},
   "source": [
    "# Activation Patching per Block, skipping it for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db84d526-4460-400e-8b22-58038cd14822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a07e5d55-6d59-4180-ba39-8bd42c52a192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6680b5bc8094a8bad5f251dae3d8860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c855c1af48514836aa4b0cf042e016a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bca8947d844383b04f979270b15ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1276.02 MB  Reserved: 1740.00 MB\n"
     ]
    }
   ],
   "source": [
    "str_tokens = model.to_str_tokens(clean_tokens[0])\n",
    "context_length = len(str_tokens)\n",
    "every_block_act_patch_result = patching.get_act_patch_block_every(\n",
    "    model, corrupted_tokens, clean_cache, ioi_metric\n",
    ").to(\"cpu\")\n",
    "\n",
    "\n",
    "# TODO : viz corresponding to Activation Patching per block\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7295a4fc-e6f1-4053-80ce-7f7c89cc9e3e",
   "metadata": {},
   "source": [
    "# Attribution Patching per Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94fd352a-df94-416b-8fba-53fd9abe5af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def get_attr_patch_block_every(attr_cache, device):\n",
    "    resid_pre_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"resid_pre\"),\n",
    "        \"layer batch pos d_model -> layer pos\",\n",
    "        \"sum\",\n",
    "    ).to(device)\n",
    "    attn_out_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"attn_out\"),\n",
    "        \"layer batch pos d_model -> layer pos\",\n",
    "        \"sum\",\n",
    "    ).to(device)\n",
    "    mlp_out_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"mlp_out\"),\n",
    "        \"layer batch pos d_model -> layer pos\",\n",
    "        \"sum\",\n",
    "    ).to(device)\n",
    "\n",
    "    every_block_attr_patch_result = torch.stack(\n",
    "        [resid_pre_attr, attn_out_attr, mlp_out_attr], dim=0\n",
    "    )\n",
    "    return every_block_attr_patch_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6dce6a26-cc47-451a-bc75-6fabfead62e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'get_attr_patch_block_every' executed in 0.0121 seconds\n",
      "Primary device: cuda:0\n",
      "GPU 0: 12207.00 MB  Allocated: 1276.02 MB  Reserved: 1740.00 MB\n"
     ]
    }
   ],
   "source": [
    "every_block_attr_patch_result = get_attr_patch_block_every(attr_cache, \"cpu\")\n",
    "check_gpu_memory()\n",
    "# TODO : viz corresponding to Attribution Patching per block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1c5caa3f-1645-4de5-bb75-5b37245f8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : viz corresponding to Activation vs Attribution Patching per block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ba8592-de04-44ee-955f-0eee971add5d",
   "metadata": {},
   "source": [
    "# Activation Patching per Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "643ff676-086e-445a-829b-9792b1c7e088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14cb6b7a6024cfb89b56c674858f657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201cccaabe374d0fbb84ca85326a4657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e68c51b310453f99775306eac379d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb51d5e7c14482bae7255d39c14df16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f9b86d00054c7bb863507326c2a73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "every_head_all_pos_act_patch_result = patching.get_act_patch_attn_head_all_pos_every(\n",
    "    model, corrupted_tokens, clean_cache, ioi_metric\n",
    ")\n",
    "\n",
    "# # TODO : viz corresponding to Activation Patching per head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d4e0ef-44cb-4755-b609-895b430f3dc1",
   "metadata": {},
   "source": [
    "# Attribution Patching per Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c36cfca3-c2ef-4dce-9f75-ee4a874fdc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'get_attr_patch_attn_head_all_pos_every' executed in 0.0183 seconds\n"
     ]
    }
   ],
   "source": [
    "@timeit\n",
    "def get_attr_patch_attn_head_all_pos_every(attr_cache, device):\n",
    "    head_out_all_pos_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"z\"),\n",
    "        \"layer batch pos head_index d_head -> layer head_index\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    head_q_all_pos_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"q\"),\n",
    "        \"layer batch pos head_index d_head -> layer head_index\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    head_k_all_pos_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"k\"),\n",
    "        \"layer batch pos head_index d_head -> layer head_index\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    head_v_all_pos_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"v\"),\n",
    "        \"layer batch pos head_index d_head -> layer head_index\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    head_pattern_all_pos_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"pattern\"),\n",
    "        \"layer batch head_index dest_pos src_pos -> layer head_index\",\n",
    "        \"sum\",\n",
    "    )\n",
    "\n",
    "    return torch.stack(\n",
    "        [\n",
    "            head_out_all_pos_attr,\n",
    "            head_q_all_pos_attr,\n",
    "            head_k_all_pos_attr,\n",
    "            head_v_all_pos_attr,\n",
    "            head_pattern_all_pos_attr,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "every_head_all_pos_attr_patch_result = get_attr_patch_attn_head_all_pos_every(\n",
    "    attr_cache, \"cpu\"\n",
    ")\n",
    "\n",
    "# TODO : viz corresponding to Attribution Patching per block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "272261b4-47dd-4184-a2fc-c5d13888e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : viz corresponding to Activation vs Attribution Patching per block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "152f164f-6cf9-4dd9-913d-0a15c9d9da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : viz corresponding to Attention for specific heads\n",
    "graph_tok_labels = [\n",
    "    f\"{tok}_{i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))\n",
    "]\n",
    "# imshow(\n",
    "#     clean_cache[\"pattern\", 5][:, 5],\n",
    "#     x=graph_tok_labels,\n",
    "#     y=graph_tok_labels,\n",
    "#     facet_col=0,\n",
    "#     title=\"Attention for Head L5H5\",\n",
    "#     facet_name=\"Prompt\",\n",
    "# )\n",
    "# imshow(\n",
    "#     clean_cache[\"pattern\", 10][:, 7],\n",
    "#     x=graph_tok_labels,\n",
    "#     y=graph_tok_labels,\n",
    "#     facet_col=0,\n",
    "#     title=\"Attention for Head L10H7\",\n",
    "#     facet_name=\"Prompt\",\n",
    "# )\n",
    "# imshow(\n",
    "#     clean_cache[\"pattern\", 11][:, 10],\n",
    "#     x=graph_tok_labels,\n",
    "#     y=graph_tok_labels,\n",
    "#     facet_col=0,\n",
    "#     title=\"Attention for Head L11H10\",\n",
    "#     facet_name=\"Prompt\",\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cdc2cd-c7d5-48cc-81ac-26fa378bfce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "every_head_by_pos_act_patch_result = patching.get_act_patch_attn_head_by_pos_every(\n",
    "    model, corrupted_tokens, clean_cache, ioi_metric\n",
    ")\n",
    "every_head_by_pos_act_patch_result = einops.rearrange(\n",
    "    every_head_by_pos_act_patch_result,\n",
    "    \"act_type layer pos head -> act_type (layer head) pos\",\n",
    ")\n",
    "# TODO : viz corresponding to Attention per head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf69a958-6e4b-4a10-9dc9-be204d5c9169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : viz corresponding to Attention for specific heads again\n",
    "graph_tok_labels = [\n",
    "    f\"{tok}_{i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))\n",
    "]\n",
    "# imshow(\n",
    "#     clean_cache[\"pattern\", 5][:, 5],\n",
    "#     x=graph_tok_labels,\n",
    "#     y=graph_tok_labels,\n",
    "#     facet_col=0,\n",
    "#     title=\"Attention for Head L5H5\",\n",
    "#     facet_name=\"Prompt\",\n",
    "# )\n",
    "# imshow(\n",
    "#     clean_cache[\"pattern\", 10][:, 7],\n",
    "#     x=graph_tok_labels,\n",
    "#     y=graph_tok_labels,\n",
    "#     facet_col=0,\n",
    "#     title=\"Attention for Head L10H7\",\n",
    "#     facet_name=\"Prompt\",\n",
    "# )\n",
    "# imshow(\n",
    "#     clean_cache[\"pattern\", 11][:, 10],\n",
    "#     x=graph_tok_labels,\n",
    "#     y=graph_tok_labels,\n",
    "#     facet_col=0,\n",
    "#     title=\"Attention for Head L11H10\",\n",
    "#     facet_name=\"Prompt\",\n",
    "# )\n",
    "\n",
    "\n",
    "# [markdown]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aee0a1-963c-4219-95a1-2be461c732a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : viz corresponding to Attention patching per head (by pos)\n",
    "\n",
    "every_head_by_pos_act_patch_result = patching.get_act_patch_attn_head_by_pos_every(\n",
    "    model, corrupted_tokens, clean_cache, ioi_metric\n",
    ")\n",
    "every_head_by_pos_act_patch_result = einops.rearrange(\n",
    "    every_head_by_pos_act_patch_result,\n",
    "    \"act_type layer pos head -> act_type (layer head) pos\",\n",
    ")\n",
    "imshow(\n",
    "    every_head_by_pos_act_patch_result,\n",
    "    facet_col=0,\n",
    "    facet_labels=[\"Output\", \"Query\", \"Key\", \"Value\", \"Pattern\"],\n",
    "    title=\"Activation Patching Per Head (By Pos)\",\n",
    "    xaxis=\"Position\",\n",
    "    yaxis=\"Layer & Head\",\n",
    "    zmax=1,\n",
    "    zmin=-1,\n",
    "    x=[f\"{tok}_{i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))],\n",
    "    y=head_out_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7523123f-e87f-4f67-98bb-5b0dd4859731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr_patch_attn_head_by_pos_every(attr_cache):\n",
    "    head_out_by_pos_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"z\"),\n",
    "        \"layer batch pos head_index d_head -> layer pos head_index\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    head_q_by_pos_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"q\"),\n",
    "        \"layer batch pos head_index d_head -> layer pos head_index\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    head_k_by_pos_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"k\"),\n",
    "        \"layer batch pos head_index d_head -> layer pos head_index\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    head_v_by_pos_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"v\"),\n",
    "        \"layer batch pos head_index d_head -> layer pos head_index\",\n",
    "        \"sum\",\n",
    "    )\n",
    "    head_pattern_by_pos_attr = einops.reduce(\n",
    "        attr_cache.stack_activation(\"pattern\"),\n",
    "        \"layer batch head_index dest_pos src_pos -> layer dest_pos head_index\",\n",
    "        \"sum\",\n",
    "    )\n",
    "\n",
    "    return torch.stack(\n",
    "        [\n",
    "            head_out_by_pos_attr,\n",
    "            head_q_by_pos_attr,\n",
    "            head_k_by_pos_attr,\n",
    "            head_v_by_pos_attr,\n",
    "            head_pattern_by_pos_attr,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "every_head_by_pos_attr_patch_result = get_attr_patch_attn_head_by_pos_every(attr_cache)\n",
    "every_head_by_pos_attr_patch_result = einops.rearrange(\n",
    "    every_head_by_pos_attr_patch_result,\n",
    "    \"act_type layer pos head -> act_type (layer head) pos\",\n",
    ")\n",
    "# TODO : viz corresponding to attribution patching per head (by pos)\n",
    "# imshow(\n",
    "#     every_head_by_pos_attr_patch_result,\n",
    "#     facet_col=0,\n",
    "#     facet_labels=[\"Output\", \"Query\", \"Key\", \"Value\", \"Pattern\"],\n",
    "#     title=\"Attribution Patching Per Head (By Pos)\",\n",
    "#     xaxis=\"Position\",\n",
    "#     yaxis=\"Layer & Head\",\n",
    "#     zmax=1,\n",
    "#     zmin=-1,\n",
    "#     x=[f\"{tok}_{i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))],\n",
    "#     y=head_out_labels,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3cc370-ed31-4dae-9ac2-e3d756cf5e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : viz corresponding to attribution vs Activation patching per head (by pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-finetuning]",
   "language": "python",
   "name": "conda-env-.conda-finetuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
