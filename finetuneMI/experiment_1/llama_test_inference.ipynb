{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeded5fd-914c-4f32-bdbf-ba13a496f600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jnainani_umass_edu/.conda/envs/finetuning/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/jnainani_umass_edu/.conda/envs/finetuning/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/jnainani_umass_edu/.conda/envs/finetuning/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/jnainani_umass_edu/codellm/MechInterpCodeLLMs/finetuneMI/experiment_1\n",
      "here\n",
      "Device is:  cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import math\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "from torch.cuda.amp import autocast\n",
    "import torch\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import json\n",
    "from transformers import LlamaConfig\n",
    "from einops import rearrange, einsum\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '/home/jnainani_umass_edu/codellm/MechInterpCodeLLMs/finetuneMI')\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "from data.data_utils import *\n",
    "from baukit.baukit import nethook\n",
    "from baukit.baukit.nethook import TraceDict\n",
    "from experiment_1.pp_utils import compute_prev_query_box_pos\n",
    "print(\"here\")\n",
    "# pretrained_model_dir = os.path.abspath(\"../../pretrained_models/llama/llama-2-7b\")\n",
    "# print(\"LLAMA 2 Weights: \", pretrained_model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4ae4f25-dad3-4533-a43b-1aab5ab8921d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jnainani_umass_edu/codellm/MechInterpCodeLLMs/finetuneMI\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "081627d7-7474-4f7f-ae8f-020d65478212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jnainani_umass_edu/codellm/MechInterpCodeLLMs/finetuneMI'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106e1b09-7487-4980-9048-cff8a3423017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff8587c4870>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 5\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(\n",
    "#     \"hf-internal-testing/llama-tokenizer\", padding_side=\"right\"\n",
    "# )\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f35fef8-4a3d-4388-9b21-0e320feb8207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd2e2cf5e7b44269cee8ae08f937805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "# with open('/home/jnainani_umass_edu/codellm/pretrained_models/llama/llama-2-7b/params.json', 'r') as f:\n",
    "#     config_dict = json.load(f)\n",
    "# config = LlamaConfig(**config_dict)\n",
    "\n",
    "# model = LlamaForCausalLM(config)\n",
    "# model_weights = torch.load('/home/jnainani_umass_edu/codellm/pretrained_models/llama/llama-2-7b/consolidated.00.pth', map_location=device)\n",
    "# model.load_state_dict(model_weights)\n",
    "\n",
    "# Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"huggyllama/llama-7b\")\n",
    "\n",
    "\n",
    "path = \"huggyllama/llama-7b\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(path).to(device)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    \"hf-internal-testing/llama-tokenizer\", padding_side=\"right\"\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5d55e-8f0c-4bb8-9a7e-0db56d749d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.nn.DataParallel(model).to(device)  # Data parallelism\n",
    "model.eval()\n",
    "\n",
    "dummy_text = [\"The document is in Box X, the pot is in Box T, the magnet is in Box A. Box X contains the \",\n",
    "              \"The document is in Box X, the pot is in Box T, the magnet is in Box A. Box T contains the \", \n",
    "              \"The document is in Box X, the pot is in Box T, the magnet is in Box A. Box A contains the \"]\n",
    "\n",
    "for dumb in dummy_text:\n",
    "    input_ids = tokenizer.encode(dumb, return_tensors=\"pt\")\n",
    "    model.eval()\n",
    "\n",
    "    # If you are using a GPU, move the input ids to the GPU\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.to('cuda')\n",
    "        model = model.to('cuda')\n",
    "\n",
    "    # Perform inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with autocast():  # Enable mixed precision\n",
    "            outputs = model.generate(input_ids, max_length=55)\n",
    "\n",
    "    # Decode predictions\n",
    "    predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"Predicted Text:\", predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b115cdd-de8e-484b-b7ed-80ab537f89f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pp_utils import (\n",
    "    get_model_and_tokenizer,\n",
    "    load_dataloader,\n",
    "    get_caches,\n",
    "    compute_topk_components,\n",
    "    patching_receiver_heads,\n",
    "    patching_sender_heads,\n",
    "    get_receiver_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3daf69e6-15a4-4932-9508-2d9c51c2d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_1.path_patching import apply_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83d01ab9-d8d1-4a2a-9fe7-b81b12ba8f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jnainani_umass_edu/codellm/MechInterpCodeLLMs/finetuneMI'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86fbd78e-5a4c-4f7d-a90e-5903e3055242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAFILE: data/dataset.jsonl\n",
      "NUM BOXES: 7\n",
      "NUM SAMPLES: 30\n",
      "VALUE FETCHER HEADS: 20\n",
      "POSITION TRANSMITTER HEADS: 5\n",
      "POSITION DETECTOR HEADS: 10\n",
      "STRUCTURAL READER HEADS: 5\n",
      "OUTPUT PATH: experiment_1/results1/path_patching/\n",
      "SEED: 20\n",
      "BATCH SIZE: 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "datafile= \"data/dataset.jsonl\"\n",
    "num_boxes= 7\n",
    "\n",
    "num_samples= 30\n",
    "n_value_fetcher= 20  # Goat / FLoat circuit: 50, Llama circuit: 20\n",
    "n_pos_trans= 5  # Goat / FLoat circuit: 20, Llama circuit: 5\n",
    "n_pos_detect = 10  # Goat / FLoat circuit: 30, Llama circuit: 10\n",
    "n_struct_read = 5  # Goat / FLoat circuit: 5, Llama circuit: 5\n",
    "output_path = \"experiment_1/results1/path_patching/\"\n",
    "seed = 20  # Goat circuit: 82, FLoat circuit: 85, Llama circuit: 20\n",
    "batch_size = 100\n",
    "\n",
    "print(f\"DATAFILE: {datafile}\")\n",
    "print(f\"NUM BOXES: {num_boxes}\")\n",
    "print(f\"NUM SAMPLES: {num_samples}\")\n",
    "print(f\"VALUE FETCHER HEADS: {n_value_fetcher}\")\n",
    "print(f\"POSITION TRANSMITTER HEADS: {n_pos_trans}\")\n",
    "print(f\"POSITION DETECTOR HEADS: {n_pos_detect}\")\n",
    "print(f\"STRUCTURAL READER HEADS: {n_struct_read}\")\n",
    "print(f\"OUTPUT PATH: {output_path}\")\n",
    "print(f\"SEED: {seed}\")\n",
    "print(f\"BATCH SIZE: {batch_size}\\n\")\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Sets the seed for reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Seed to use.\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    transformers.set_seed(seed)\n",
    "    \n",
    "set_seed(seed)\n",
    "\n",
    "dataloader = load_dataloader(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        datafile=datafile,\n",
    "        num_samples=num_samples,\n",
    "        num_boxes=num_boxes,\n",
    "        batch_size=batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2500e8-7909-4d18-87ee-96ba66b7e943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a70b28b7-39d1-4c9c-8ed3-8485b36ebca9",
   "metadata": {},
   "source": [
    "## Step 1: Compute clean and corrupted cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24fcf9c5-2cc6-468b-a0dd-022bc661ab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clean cache: 1it [00:04,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEAN CACHE COMPUTED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt cache: 1it [00:03,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRUPT CACHE COMPUTED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "        clean_cache,\n",
    "        corrupt_cache,\n",
    "        clean_logit_outputs,\n",
    "        _,\n",
    "        hook_points,\n",
    "    ) = get_caches(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1979e42c-8937-4c44-9d23-35451d94b5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: TraceDict([('model.layers.0.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff93ae5d8a0>),\n",
       "            ('model.layers.1.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff845278a90>),\n",
       "            ('model.layers.2.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d48dff0>),\n",
       "            ('model.layers.3.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d48e200>),\n",
       "            ('model.layers.4.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d48de70>),\n",
       "            ('model.layers.5.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d48dba0>),\n",
       "            ('model.layers.6.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d48d930>),\n",
       "            ('model.layers.7.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d48d720>),\n",
       "            ('model.layers.8.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d48d420>),\n",
       "            ('model.layers.9.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d48ccd0>),\n",
       "            ('model.layers.10.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d48c2e0>),\n",
       "            ('model.layers.11.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d48eef0>),\n",
       "            ('model.layers.12.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d48f4f0>),\n",
       "            ('model.layers.13.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d48f760>),\n",
       "            ('model.layers.14.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d48f9d0>),\n",
       "            ('model.layers.15.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d48fc40>),\n",
       "            ('model.layers.16.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d48feb0>),\n",
       "            ('model.layers.17.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d3f0160>),\n",
       "            ('model.layers.18.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d3f03d0>),\n",
       "            ('model.layers.19.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d3f0640>),\n",
       "            ('model.layers.20.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d3f08b0>),\n",
       "            ('model.layers.21.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d3f0b20>),\n",
       "            ('model.layers.22.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d3f0d90>),\n",
       "            ('model.layers.23.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d3f1000>),\n",
       "            ('model.layers.24.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d3f1270>),\n",
       "            ('model.layers.25.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d3f13f0>),\n",
       "            ('model.layers.26.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d3f1660>),\n",
       "            ('model.layers.27.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d3f18d0>),\n",
       "            ('model.layers.28.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d3f1b40>),\n",
       "            ('model.layers.29.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d3f1db0>),\n",
       "            ('model.layers.30.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d3f2020>),\n",
       "            ('model.layers.31.self_attn.o_proj',\n",
       "             <baukit.baukit.nethook.Trace at 0x7ff83d3f2290>)])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf6629a-776d-482b-8f9b-e4722f583f25",
   "metadata": {},
   "source": [
    "## Compute Value Fetcher Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228f64c3-1c48-4de6-a5a6-b957892b97d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTING VALUE FETCHER HEADS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 12/32 [14:31<24:15, 72.75s/it]"
     ]
    }
   ],
   "source": [
    "print(\"COMPUTING VALUE FETCHER HEADS...\")\n",
    "patching_scores = apply_pp(\n",
    "    model=model,\n",
    "    clean_cache=clean_cache,\n",
    "    corrupt_cache=corrupt_cache,\n",
    "    dataloader=dataloader,\n",
    "    receiver_heads=[],\n",
    "    receiver_layers=[],\n",
    "    clean_logit_outputs=clean_logit_outputs,\n",
    "    hook_points=hook_points,\n",
    "    rel_pos=0,\n",
    ")\n",
    "torch.save(patching_scores, output_path + \"value_fetcher.pt\")\n",
    "value_fetcher_heads = compute_topk_components(\n",
    "    patching_scores=patching_scores, k=n_value_fetcher, largest=False\n",
    ")\n",
    "print(f\"VALUE FETCHER HEADS: {value_fetcher_heads}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31fb5e-a282-492b-a5c4-28c3c47cd16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Are you still here?\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fc5fb-615d-4114-b6ea-59d29d950449",
   "metadata": {},
   "source": [
    "# Code LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69223620-0d85-4440-879c-6bf5da1b6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-finetuning]",
   "language": "python",
   "name": "conda-env-.conda-finetuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
