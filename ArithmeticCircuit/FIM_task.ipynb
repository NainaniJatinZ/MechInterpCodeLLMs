{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d01dc0-ae3e-4e1b-974e-6c7e83ceacf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in /home/jnainani_umass_edu/.conda/envs/finetuning/lib/python3.10/site-packages (5.26.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae84b947-8452-4a91-88cb-c3d6c54626de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jnainani_umass_edu/.conda/envs/finetuning/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/jnainani_umass_edu/.conda/envs/finetuning/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/jnainani_umass_edu/.conda/envs/finetuning/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, CodeLlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b1213f-0df9-468e-827d-79f18da7bc73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a734cadd14af4201a15a5ff29f0429d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = CodeLlamaTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "PROMPT = '''def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\" <FILL_ME>\n",
    "    return result\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff9e80c-a4e9-4745-8853-dfba0001dc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a716db20cefa45558ebb04ed6c25a3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de082308a09d48e99177fcd9a771e3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fbedbe436847aea5aabcd09de76cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e5d1eceb8f47ffb42814cb5a039b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c24bd8f9ce47deb4a6bd082945a2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/646 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5260e32e2b1144f9bdbae059aa4d0699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345fbc85616e477c99bbba6401308f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jnainani_umass_edu/.conda/envs/finetuning/lib/python3.10/site-packages/huggingface_hub/file_download.py:979: UserWarning: Not enough free disk space to download the file. The expected file size is: 9976.70 MB. The target location /home/jnainani_umass_edu/.cache/huggingface/hub only has 5242.88 MB free disk space.\n",
      "  warnings.warn(\n",
      "/home/jnainani_umass_edu/.conda/envs/finetuning/lib/python3.10/site-packages/huggingface_hub/file_download.py:979: UserWarning: Not enough free disk space to download the file. The expected file size is: 9976.70 MB. The target location /home/jnainani_umass_edu/.cache/huggingface/hub/models--codellama--CodeLlama-7b-Instruct-hf/blobs only has 5242.88 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a0be5247074a05a77608f89f410faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = CodeLlamaTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "\n",
    "PROMPT = '''def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\" <FILL_ME>\n",
    "    return result\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82cc08ae-442c-4b9a-a396-5941e94e90ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(PROMPT, return_tensors=\"pt\")[\"input_ids\"]\n",
    "generated_ids = model.generate(input_ids, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47de2068-e44d-4a4d-964a-de12f7083ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT2 = '''def remove_non_ascii(s: str) -> str:\n",
    "    \"\"\"\n",
    "    return result\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc53487b-94af-4d4a-a721-37402dfe3328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 32007,   822,  3349, 29918,  5464, 29918,   294, 18869, 29898,\n",
      "         29879, 29901,   851, 29897,  1599,   851, 29901,    13,  1678,  9995,\n",
      "         29871, 32008,    13,  1678,   736,  1121,    13, 32009]])\n",
      "tensor([[    1,   822,  3349, 29918,  5464, 29918,   294, 18869, 29898, 29879,\n",
      "         29901,   851, 29897,  1599,   851, 29901,    13,  1678,  9995,    13,\n",
      "          1678,   736,  1121,    13]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(PROMPT, return_tensors=\"pt\")[\"input_ids\"]\n",
    "print(input_ids)\n",
    "\n",
    "print(tokenizer(PROMPT2, return_tensors=\"pt\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8e33d31-4fd1-48e2-b519-e5b6e40bd065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PRE>', '', '<SUF>', '<MID>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([32007,29871, 32008, 32009])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77845025-2b67-42fb-86fa-87abb795990b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def remove_non_ascii(s: str) -> str:\n",
      "    \"\"\" Remove non-ASCII characters from a string.\n",
      "\n",
      "    Args:\n",
      "        s: The string to remove non-ASCII characters from.\n",
      "\n",
      "    Returns:\n",
      "        The string with non-ASCII characters removed.\n",
      "    \"\"\"\n",
      "    result = \"\"\n",
      "    for c in s:\n",
      "        if ord(c) < 128:\n",
      "            result += c\n",
      "    return result\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]\n",
    "print(PROMPT.replace(\"<FILL_ME>\", filling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e8380a0-992e-4a9d-af35-19becfb23122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&\n"
     ]
    }
   ],
   "source": [
    "PROMPT = '''#include <iostream>\n",
    "\n",
    "void modifyValue(int<FILL_ME>value) {\n",
    "    value += 10;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int num = 5;\n",
    "    modifyValue(num);\n",
    "    std::cout << \"Modified value: \" << num << std::endl;\n",
    "    return 0;\n",
    "}\n",
    "'''\n",
    "\n",
    "input_ids = tokenizer(PROMPT, return_tensors=\"pt\")[\"input_ids\"]\n",
    "generated_ids = model.generate(input_ids, max_new_tokens=1)\n",
    "filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]\n",
    "print(filling)\n",
    "# print(PROMPT.replace(\"<FILL_ME>\", filling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c688092-d991-4f40-a362-4642ff4b8169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str):\n"
     ]
    }
   ],
   "source": [
    "PROMPT = '''def palindrome(s: <FILL_ME> ):\n",
    "return s[::-1] == s\n",
    "'''\n",
    "\n",
    "input_ids = tokenizer(PROMPT, return_tensors=\"pt\")[\"input_ids\"]\n",
    "generated_ids = model.generate(input_ids, max_new_tokens=2)\n",
    "filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]\n",
    "print(filling)\n",
    "# print(PROMPT.replace(\"<FILL_ME>\", filling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d770d1b1-e0fc-4ae5-87fb-9feeb85827df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   13, 29898, 29874, 29974, 29890]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids[:, input_ids.shape[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb459739-15ab-4600-a072-77cfcf3bbc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "O(a,\n"
     ]
    }
   ],
   "source": [
    "PROMPT = '''Custom Entities:\n",
    "def X(a, b):\n",
    "    return a + b\n",
    "def Y(a, b):\n",
    "    return a - b\n",
    "def Z(a, b):\n",
    "    return a * b\n",
    "def W(a, b):\n",
    "    return a / b\n",
    "def G(a, b):\n",
    "    return a ** b\n",
    "def H(a, b):\n",
    "    return a % b\n",
    "def F(a, b):\n",
    "    return a < b\n",
    "def K(a, b):\n",
    "    return a > b\n",
    "def O(a, b):\n",
    "    return a <= b\n",
    "def P(a, b):\n",
    "    return a >= b\n",
    "def D(a, b):\n",
    "    return a == b\n",
    "\n",
    "the name of custom entity that uses \"<=\" operator is \n",
    "'''\n",
    "\n",
    "input_ids = tokenizer(PROMPT, return_tensors=\"pt\")[\"input_ids\"]\n",
    "generated_ids = model.generate(input_ids, max_new_tokens=5)\n",
    "filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]\n",
    "print(filling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f6ffa-56b9-4493-ba9e-cf33afd643a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cdd732-6b88-413c-a07e-fca7f803edb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-finetuning]",
   "language": "python",
   "name": "conda-env-.conda-finetuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
